{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190316_tfagents_berater-v12_dqn_openai.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "w3OdHyWEEEwy",
        "Usj9iWTskQ3t",
        "4GlYjZ3xkQ38"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbuild/tfagents/blob/master/190316_tfagents_berater_v12_dqn_openai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v13\n",
        "\n",
        "## Changes from v12 (work in progress)\n",
        "* port to tfagents with dqn\n",
        "* includes original openai beraterv12 implementation"
      ]
    },
    {
      "metadata": {
        "id": "PQiN7IVMS6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_iterations = 20000  # @param\n",
        "\n",
        "initial_collect_steps = 50  # @param\n",
        "collect_steps_per_iteration = 1  # @param\n",
        "replay_buffer_capacity = 100000  # @param\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "batch_size = 64  # @param\n",
        "learning_rate = 1e-3  # @param\n",
        "log_interval = 500  # @param\n",
        "\n",
        "num_eval_episodes = 10  # @param\n",
        "eval_interval = 1000  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-agents"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-agents-nightly > /dev/null\n",
        "!pip install tf-nightly > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Environment"
      ]
    },
    {
      "metadata": {
        "id": "sQ8Nfk3MKgLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HQyb_Aq8Kg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsJ6zcXvwN53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-olom0nwiSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Berater Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "3plH2u3Swotj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "        if self.stepCount >= 200:\n",
        "          if BeraterEnv.showDone:\n",
        "            print(\"Done: stepCount >= 200\")\n",
        "          done = True\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J54w2URZIme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYaTAvAyYO-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "yhI9abUVYNrU",
        "colab_type": "code",
        "outputId": "3f6f3946-be12-46bf-de18-65e9c576a63b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if not 'isEnvRegistered' in locals():\n",
        "  env_id=\"Berater-v1\"\n",
        "  gym.envs.registration.register(id=env_id,entry_point=BeraterEnv,max_episode_steps=1000)\n",
        "  isEnvRegistered=True\n",
        "  print(\"Berater registered as '\" + env_id + \"'\")\n",
        "else:\n",
        "  print(\"Already registered\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater registered as 'Berater-v1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX8eJGcbOJ30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF-Agents: Train, enjoy, evaluate"
      ]
    },
    {
      "metadata": {
        "id": "bzoq0VM85p46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports & Helpers"
      ]
    },
    {
      "metadata": {
        "id": "5ofYknQFRkRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import trajectory\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiP6UgA65163",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZoGMeUBr2tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup tf-agent envs, models, policies & agents"
      ]
    },
    {
      "metadata": {
        "id": "sldmm0LmcINW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name=\"Berater-v1\"\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYOUS69Mh3tj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FuiFfYjh9-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.compat.v2.Variable(0)\n",
        "\n",
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "tf_agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hss9CFs2iEt9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CNLGWERiStH",
        "colab_type": "code",
        "outputId": "2265c575-91b3-4bfd-a03b-af39bf9b2354",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "avg_rnd_return = compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "print(\"avgerage return with random policy={}\".format(avg_rnd_return))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avgerage return with random policy=-6.630001068115234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVcuWB_FsHa8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup replay buffer (train_env)"
      ]
    },
    {
      "metadata": {
        "id": "dYdP4HYHrgGq",
        "colab_type": "code",
        "outputId": "54dd6d69-9fbc-4fee-d282-c055e70ab33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "print(\"replay buffer batch_size=\" + str(train_env.batch_size) + \\\n",
        "      \" max_length=\" + str(replay_buffer_capacity) + \\\n",
        "      \" size=\" + str(replay_buffer.gather_all().observation.shape[1])\n",
        "     )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay buffer batch_size=1 max_length=100000 size=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UD3yzROJrwUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "  # print(\"collect_step: trajectory=\" + str(traj))\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  replay_buffer.add_batch(traj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDSOinfuVk7u",
        "colab_type": "code",
        "outputId": "b776a573-a13f-4ede-be3a-1974ac1a6a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay_buffer size=0 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDRID-p0QOKo",
        "colab_type": "code",
        "outputId": "0b2010a7-4b2a-4381-cbd2-9906e8585943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"initial_collect_steps=\" + str(initial_collect_steps))\n",
        "for _ in range(initial_collect_steps):\n",
        "  collect_step(train_env, random_policy)\n",
        "\n",
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_collect_steps=50\n",
            "replay_buffer size=50 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bCHhtAfPsk9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwMsD8ATcBBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(iterator.next()[0].action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myOp6C8ks0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the agent"
      ]
    },
    {
      "metadata": {
        "id": "YnptN6_dNJBL",
        "colab_type": "code",
        "outputId": "e8ca275a-2c2f-4749-c120-ea4afbca02a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "print(\"retuns={} (before training) num_eval_episodes={}\".format(returns,num_eval_episodes))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "retuns=[-24.814976] num_eval_episodes=10\n",
            "CPU times: user 6.83 s, sys: 199 ms, total: 7.02 s\n",
            "Wall time: 6.94 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ui4l-FVIsyxp",
        "colab_type": "code",
        "outputId": "47249162-2a0f-4c0d-fd53-62b5b24f4720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3924
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect one step using collect_policy and save to the replay buffer.\n",
        "  collect_step(train_env, tf_agent.collect_policy)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = tf_agent.train(experience)\n",
        "\n",
        "  step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 10: loss = 8786.876953125\n",
            "step = 20: loss = 9771.107421875\n",
            "step = 30: loss = 4586.8759765625\n",
            "step = 40: loss = 13560.931640625\n",
            "step = 50: loss = 20443.58984375\n",
            "step = 60: loss = 24458.505859375\n",
            "step = 70: loss = 20240.396484375\n",
            "step = 80: loss = 38077.3671875\n",
            "step = 90: loss = 121028.8828125\n",
            "step = 100: loss = 28873.232421875\n",
            "step = 100: Average Return = -32.994956970214844\n",
            "step = 110: loss = 132478.515625\n",
            "step = 120: loss = 265384.125\n",
            "step = 130: loss = 174302.09375\n",
            "step = 140: loss = 204785.203125\n",
            "step = 150: loss = 392971.65625\n",
            "step = 160: loss = 1118498.125\n",
            "step = 170: loss = 128556.078125\n",
            "step = 180: loss = 1021347.5\n",
            "step = 190: loss = 1017544.125\n",
            "step = 200: loss = 935455.0\n",
            "step = 200: Average Return = -32.816627502441406\n",
            "step = 210: loss = 253966.0625\n",
            "step = 220: loss = 1011911.5\n",
            "step = 230: loss = 1371143.5\n",
            "step = 240: loss = 1714275.375\n",
            "step = 250: loss = 1253881.5\n",
            "step = 260: loss = 1582263.625\n",
            "step = 270: loss = 4020013.5\n",
            "step = 280: loss = 2624180.75\n",
            "step = 290: loss = 3352291.5\n",
            "step = 300: loss = 4245718.0\n",
            "step = 300: Average Return = -29.69663429260254\n",
            "step = 310: loss = 1307025.0\n",
            "step = 320: loss = 4626093.0\n",
            "step = 330: loss = 4170480.0\n",
            "step = 340: loss = 11952140.0\n",
            "step = 350: loss = 10188478.0\n",
            "step = 360: loss = 43365216.0\n",
            "step = 370: loss = 19996302.0\n",
            "step = 380: loss = 10639647.0\n",
            "step = 390: loss = 15507037.0\n",
            "step = 400: loss = 4432399.0\n",
            "step = 400: Average Return = -32.83329391479492\n",
            "step = 410: loss = 16313344.0\n",
            "step = 420: loss = 23717518.0\n",
            "step = 430: loss = 7297332.0\n",
            "step = 440: loss = 21992106.0\n",
            "step = 450: loss = 50207432.0\n",
            "step = 460: loss = 13867440.0\n",
            "step = 470: loss = 78858856.0\n",
            "step = 480: loss = 89089848.0\n",
            "step = 490: loss = 41186948.0\n",
            "step = 500: loss = 32001952.0\n",
            "step = 500: Average Return = -32.799957275390625\n",
            "step = 510: loss = 43021228.0\n",
            "step = 520: loss = 31349676.0\n",
            "step = 530: loss = 13714183.0\n",
            "step = 540: loss = 36608584.0\n",
            "step = 550: loss = 48261976.0\n",
            "step = 560: loss = 24237324.0\n",
            "step = 570: loss = 154166752.0\n",
            "step = 580: loss = 12627288.0\n",
            "step = 590: loss = 41063416.0\n",
            "step = 600: loss = 89513912.0\n",
            "step = 600: Average Return = -32.766624450683594\n",
            "step = 610: loss = 19726758.0\n",
            "step = 620: loss = 93673776.0\n",
            "step = 630: loss = 15111919.0\n",
            "step = 640: loss = 33571488.0\n",
            "step = 650: loss = 257470944.0\n",
            "step = 660: loss = 60210728.0\n",
            "step = 670: loss = 11045636.0\n",
            "step = 680: loss = 496641952.0\n",
            "step = 690: loss = 69357616.0\n",
            "step = 700: loss = 31804480.0\n",
            "step = 700: Average Return = -32.88329315185547\n",
            "step = 710: loss = 14901768.0\n",
            "step = 720: loss = 37532372.0\n",
            "step = 730: loss = 46739100.0\n",
            "step = 740: loss = 123677384.0\n",
            "step = 750: loss = 12334758.0\n",
            "step = 760: loss = 50911568.0\n",
            "step = 770: loss = 81352880.0\n",
            "step = 780: loss = 514778336.0\n",
            "step = 790: loss = 21662116.0\n",
            "step = 800: loss = 49028176.0\n",
            "step = 800: Average Return = -32.86662673950195\n",
            "step = 810: loss = 506479744.0\n",
            "step = 820: loss = 20434974.0\n",
            "step = 830: loss = 64499492.0\n",
            "step = 840: loss = 81941192.0\n",
            "step = 850: loss = 9724080.0\n",
            "step = 860: loss = 1005531648.0\n",
            "step = 870: loss = 43969152.0\n",
            "step = 880: loss = 15017164.0\n",
            "step = 890: loss = 44548968.0\n",
            "step = 900: loss = 41282640.0\n",
            "step = 900: Average Return = -25.023303985595703\n",
            "step = 910: loss = 8685308.0\n",
            "step = 920: loss = 12491042.0\n",
            "step = 930: loss = 16982270.0\n",
            "step = 940: loss = 15715622.0\n",
            "step = 950: loss = 8980940.0\n",
            "step = 960: loss = 46302624.0\n",
            "step = 970: loss = 32265924.0\n",
            "step = 980: loss = 840266816.0\n",
            "step = 990: loss = 68240008.0\n",
            "step = 1000: loss = 15762344.0\n",
            "step = 1000: Average Return = -9.621661186218262\n",
            "step = 1010: loss = 439010624.0\n",
            "step = 1020: loss = 87663200.0\n",
            "step = 1030: loss = 218648896.0\n",
            "step = 1040: loss = 202173376.0\n",
            "step = 1050: loss = 29458648.0\n",
            "step = 1060: loss = 10771408.0\n",
            "step = 1070: loss = 16287478.0\n",
            "step = 1080: loss = 20240430.0\n",
            "step = 1090: loss = 27078710.0\n",
            "step = 1100: loss = 102602448.0\n",
            "step = 1100: Average Return = -17.498315811157227\n",
            "step = 1110: loss = 1155281280.0\n",
            "step = 1120: loss = 47816484.0\n",
            "step = 1130: loss = 7972034.0\n",
            "step = 1140: loss = 2499646.0\n",
            "step = 1150: loss = 736082176.0\n",
            "step = 1160: loss = 819576256.0\n",
            "step = 1170: loss = 53127488.0\n",
            "step = 1180: loss = 1590429.75\n",
            "step = 1190: loss = 25382758.0\n",
            "step = 1200: loss = 8244918.0\n",
            "step = 1200: Average Return = -17.079980850219727\n",
            "step = 1210: loss = 26825292.0\n",
            "step = 1220: loss = 409553440.0\n",
            "step = 1230: loss = 11926730.0\n",
            "step = 1240: loss = 50685540.0\n",
            "step = 1250: loss = 9701671.0\n",
            "step = 1260: loss = 7891367.0\n",
            "step = 1270: loss = 7260508.0\n",
            "step = 1280: loss = 26186744.0\n",
            "step = 1290: loss = 9610503.0\n",
            "step = 1300: loss = 7440663.5\n",
            "step = 1300: Average Return = -10.978338241577148\n",
            "step = 1310: loss = 23635584.0\n",
            "step = 1320: loss = 10044748.0\n",
            "step = 1330: loss = 13182850.0\n",
            "step = 1340: loss = 13261531.0\n",
            "step = 1350: loss = 5049171.0\n",
            "step = 1360: loss = 4435626.0\n",
            "step = 1370: loss = 59725172.0\n",
            "step = 1380: loss = 1067185408.0\n",
            "step = 1390: loss = 5346378.0\n",
            "step = 1400: loss = 169565264.0\n",
            "step = 1400: Average Return = -27.643301010131836\n",
            "step = 1410: loss = 10057436.0\n",
            "step = 1420: loss = 369443552.0\n",
            "step = 1430: loss = 88949312.0\n",
            "step = 1440: loss = 31124896.0\n",
            "step = 1450: loss = 206409184.0\n",
            "step = 1460: loss = 16785716.0\n",
            "step = 1470: loss = 32313492.0\n",
            "step = 1480: loss = 343081312.0\n",
            "step = 1490: loss = 22589776.0\n",
            "step = 1500: loss = 4258333.0\n",
            "step = 1500: Average Return = -17.096647262573242\n",
            "step = 1510: loss = 17971784.0\n",
            "step = 1520: loss = 24652828.0\n",
            "step = 1530: loss = 5966577.0\n",
            "step = 1540: loss = 8797630.0\n",
            "step = 1550: loss = 8127675.0\n",
            "step = 1560: loss = 18913454.0\n",
            "step = 1570: loss = 4812489.0\n",
            "step = 1580: loss = 42575064.0\n",
            "step = 1590: loss = 713673856.0\n",
            "step = 1600: loss = 7027306.0\n",
            "step = 1600: Average Return = -9.554993629455566\n",
            "step = 1610: loss = 334003424.0\n",
            "step = 1620: loss = 77601512.0\n",
            "step = 1630: loss = 28923442.0\n",
            "step = 1640: loss = 13989408.0\n",
            "step = 1650: loss = 14751904.0\n",
            "step = 1660: loss = 11001449.0\n",
            "step = 1670: loss = 823843840.0\n",
            "step = 1680: loss = 22202146.0\n",
            "step = 1690: loss = 4887234.0\n",
            "step = 1700: loss = 13037350.0\n",
            "step = 1700: Average Return = -17.16331672668457\n",
            "step = 1710: loss = 30835144.0\n",
            "step = 1720: loss = 27854048.0\n",
            "step = 1730: loss = 69147240.0\n",
            "step = 1740: loss = 733124096.0\n",
            "step = 1750: loss = 44458176.0\n",
            "step = 1760: loss = 26253824.0\n",
            "step = 1770: loss = 15145158.0\n",
            "step = 1780: loss = 379131392.0\n",
            "step = 1790: loss = 14691385.0\n",
            "step = 1800: loss = 6967313.0\n",
            "step = 1800: Average Return = -9.028345108032227\n",
            "step = 1810: loss = 5091190.5\n",
            "step = 1820: loss = 39594952.0\n",
            "step = 1830: loss = 21378082.0\n",
            "step = 1840: loss = 17250608.0\n",
            "step = 1850: loss = 3687598.5\n",
            "step = 1860: loss = 6744347.5\n",
            "step = 1870: loss = 12242722.0\n",
            "step = 1880: loss = 30583840.0\n",
            "step = 1890: loss = 15802428.0\n",
            "step = 1900: loss = 8176624.0\n",
            "step = 1900: Average Return = -10.324999809265137\n",
            "step = 1910: loss = 165872576.0\n",
            "step = 1920: loss = 70503352.0\n",
            "step = 1930: loss = 2421493.5\n",
            "step = 1940: loss = 32957604.0\n",
            "step = 1950: loss = 144572128.0\n",
            "step = 1960: loss = 665365504.0\n",
            "step = 1970: loss = 9488442.0\n",
            "step = 1980: loss = 27874352.0\n",
            "step = 1990: loss = 5566320.0\n",
            "step = 2000: loss = 28577220.0\n",
            "step = 2000: Average Return = -10.643335342407227\n",
            "CPU times: user 2min 36s, sys: 4.52 s, total: 2min 41s\n",
            "Wall time: 2min 38s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ty7ul0Lk6nou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training rewards"
      ]
    },
    {
      "metadata": {
        "id": "6vrAtGt3ck4Z",
        "colab_type": "code",
        "outputId": "9f09dd8c-e0ab-4179-c688-8b87236786b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"num_iterations={} eval_interval={} tf_agent: train_step_counter={} train_sequence_length={}\".format(\\\n",
        "                 num_iterations, \\\n",
        "                 eval_interval, \\\n",
        "                 tf_agent.train_step_counter.numpy(), \\\n",
        "                 tf_agent.train_sequence_length))\n",
        "print(\"returns={}\".format(returns))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_iterations=2000 eval_interval=100 tf_agent: train_step_counter=2000 train_sequence_length=2\n",
            "returns=[-24.814976, -32.994957, -32.816628, -29.696634, -32.833294, -32.799957, -32.766624, -32.883293, -32.866627, -25.023304, -9.621661, -17.498316, -17.07998, -10.978338, -27.643301, -17.096647, -9.554994, -17.163317, -9.028345, -10.325, -10.643335]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a1d6e0327e23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_iterations={} eval_interval={} tf_agent: train_step_counter={} train_sequence_length={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m                 \u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0meval_interval\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                  \u001b[0mtf_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"returns={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"steps={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'steps' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "58gwqW92M_d_",
        "colab_type": "code",
        "outputId": "314f732b-83bb-492e-8295-2664dede110e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-34.19328756332398, 250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFcCAYAAAAzhzxOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8U3W+//FX2rR0odAthZZFkEU7\nUAplkUV2kWVcAAE3ZJyrP1dUFEVERrnOjArMzGMUmVFxuQpeh7GOI9xRQUXWgaoUSmFUVqW0lSZt\naUv3Juf3RyFSaUnRJs0p7+fjwSPNSXL6+ZAm73y/5+Qci2EYBiIiImJKAc1dgIiIiPx0CnIRERET\nU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTGrN1e+ZMkSdu7cSU1NDXfeeScbNmxg3759REZGAnDb\nbbcxatQo1qxZwxtvvEFAQAAzZsxg+vTp3ixLRESkxfBakO/YsYMDBw6wevVqCgsLmTJlCoMHD+ah\nhx5i9OjR7vuVlZWxfPlyUlNTCQoKYtq0aYwbN84d9iIiItIwrwX5wIED6dOnDwBt2rShvLwcp9N5\n1v0yMjJISkoiIiICgJSUFNLT0xkzZoy3ShMREWkxvLaNPDAwkLCwMABSU1MZMWIEgYGBrFq1ilmz\nZvHggw9SUFCAw+EgOjra/bjo6Gjsdru3yhIREWlRvLqNHOCTTz4hNTWV1157jb179xIZGUliYiIv\nv/wyL7zwAv369atz/8YcMbamxonVGuitkkVEREzDq0G+ZcsWXnzxRV555RUiIiIYMmSI+7YxY8aw\naNEixo8fj8PhcC/Py8ujb9++51xvYWFZk9Zps0Vgt5c06Tqbi3rxTy2ll5bSB6gXf9RS+oCm78Vm\ni2jwNq9NrZeUlLBkyRJeeukl945r9913H1lZWQCkpaXRo0cPkpOTyczMpLi4mNLSUtLT0xkwYIC3\nyhIREWlRvDYi/+CDDygsLGTOnDnuZVOnTmXOnDmEhoYSFhbGM888Q0hICHPnzuW2227DYrFw7733\nund8ExERkXOzmPE0pk099aLpHP+kXvxPS+kD1Is/ail9QAuZWhcRERHvU5CLiIiYmIJcRETExBTk\nIiIiJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIiJqYg\nFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTEF\nuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTEFuYiIiIkp\nyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTEFuYiIiIkpyEVERExM\nQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIiJmb15sqXLFnCzp07qamp4c477yQpKYl58+bh\ndDqx2WwsXbqU4OBg1qxZwxtvvEFAQAAzZsxg+vTp3ixLRESkxfBakO/YsYMDBw6wevVqCgsLmTJl\nCkOGDOGmm25i4sSJ/OlPfyI1NZXJkyezfPlyUlNTCQoKYtq0aYwbN47IyEhvlSYiItJieG1qfeDA\ngTz33HMAtGnThvLyctLS0hg7diwAo0ePZvv27WRkZJCUlERERAQhISGkpKSQnp7urbJERERaFK+N\nyAMDAwkLCwMgNTWVESNGsHXrVoKDgwGIiYnBbrfjcDiIjo52Py46Ohq73X7OdUdFhWG1BjZpvTZb\nRJOurzmpF//UUnppKX2AevFHLaUP8F0vXt1GDvDJJ5+QmprKa6+9xpVXXulebhhGvfdvaPmZCgvL\nmqw+qP3PtttLmnSdzUW9+KeW0ktL6QPUiz9qKX1A0/dyrg8FXt1rfcuWLbz44ousWLGCiIgIwsLC\nqKioAOD48ePExcURFxeHw+FwPyYvL4+4uDhvliUiItJieC3IS0pKWLJkCS+99JJ7x7WhQ4eybt06\nANavX8/w4cNJTk4mMzOT4uJiSktLSU9PZ8CAAd4qS0REpEXx2tT6Bx98QGFhIXPmzHEve/bZZ1m4\ncCGrV68mISGByZMnExQUxNy5c7ntttuwWCzce++9RES0nG0kIiIi3mQxGrNR2s809TYUbZfxT+rF\n/7SUPkC9+KOW0ge0oG3kIiIi4l0KchERERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiI\niSnIRURETExBLiIiYmIKchERERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiSnIRURE\nTExBLiIiYmIKchERERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiSnIRURETExBLiIi\nYmIKchERERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiSnIRURETExBLiIiYmIKchER\nERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiSnIRURETExBLiIiYmIKchERERNTkIuI\niJiYV4N8//79XHHFFaxatQqA+fPnc/XVV3PLLbdwyy23sHHjRgDWrFnDddddx/Tp03nnnXe8WZKI\niEiLYvXWisvKyvjtb3/LkCFD6ix/6KGHGD16dJ37LV++nNTUVIKCgpg2bRrjxo0jMjLSW6WJiIi0\nGF4bkQcHB7NixQri4uLOeb+MjAySkpKIiIggJCSElJQU0tPTvVWWiIhIi+K1ILdarYSEhJy1fNWq\nVcyaNYsHH3yQgoICHA4H0dHR7tujo6Ox2+3eKktERKRF8drUen2uvfZaIiMjSUxM5OWXX+aFF16g\nX79+de5jGIbH9URFhWG1BjZpbTZbRJOurzmpF//UUnppKX2AevFHLaUP8F0vPg3yM7eXjxkzhkWL\nFjF+/HgcDod7eV5eHn379j3negoLy5q0LpstAru9pEnX2VzUi39qKb20lD5AvfijltIHNH0v5/pQ\n4NOvn913331kZWUBkJaWRo8ePUhOTiYzM5Pi4mJKS0tJT09nwIABvixLRETEtLw2It+7dy+LFy8m\nOzsbq9XKunXrmDlzJnPmzCE0NJSwsDCeeeYZQkJCmDt3LrfddhsWi4V7772XiIiWM7UiIiLiTV4L\n8t69e7Ny5cqzlo8fP/6sZRMmTGDChAneKkVERKTF0pHdRERETExBLiIiYmIep9YPHDjAO++8Q1FR\nUZ2vhi1ZssSrhYmIiIhnHoN8zpw5TJw4kcTERF/UIyIiIufBY5DHxsYye/ZsX9QiIiIi58njNvIR\nI0awdetWqqqqcLlc7n8iIiLS/DyOyP/6179y8uRJLBYLUHsIVYvFwldffeX14kREROTcPAb5559/\nTkCAdm4XERHxRx4T+le/+pUv6hAREZGfwOOIPDExkeeee45+/foRFBTkXn7mCVBERESkeXgM8tPb\nwr/88kv3MovFoiAXERHxAx6DvL7jpYuIiIh/8BjkN910k3uP9TO99dZbXilIREREGq9RR3Y7rbq6\nmh07dhAWFubVokRERKRxPAb5oEGD6lwfNmwY/+///T+vFSQiIiKN5zHIs7Ky6lzPzc3lyJEjXitI\nREREGs9jkJ/5PXKLxUJERISOvS4iIuInPAb5ihUr6NatW51lu3fv9lpBIiIi0ngNHtmtuLiYo0eP\nsmDBArKystz/Dh8+zKOPPurLGkVERKQBDY7Id+3axRtvvMFXX31VZ3o9ICCAyy+/3CfFiYiIyLk1\nGOQjR45k5MiRvP3229x4442+rElEREQayeNJUyZOnMjixYt55JFHANiwYQMFBQVeL0xEREQ88xjk\nv/nNb4iPj3d/Da2qqkrbyEVERPyExyAvKChg1qxZ7jOfTZgwgYqKCq8XJiIiIp55DHKoPTTr6eOt\nOxwOysrKvFqUiIiINI7H75HffPPNTJs2Dbvdzl133UVmZiaPP/64L2oTERERDzwG+aRJk0hJSWHX\nrl0EBwfz1FNPERcX54vaRERExINzBvmhQ4c4ePAgSUlJTJw40b38ww8/rHNdREREmkeD28jffvtt\n7r77btauXcuMGTPYtm0bBQUF3H///bz++uu+rFFEREQa0OCI/L333mPNmjWEhISQlZXF7bffTk1N\nDb/61a+45ZZbfFmjiIiINKDBIG/VqhUhISEAdOrUidDQUP7yl7+QkJDgs+JERETk3BqcWj/9dbPT\n2rRpoxAXERHxMw2OyCsrK91Hc6vveqdOnbxbmYiIiHjUYJDb7XZuvfVWDMNwLzt9FjSLxcKnn37q\n/epERETknBoM8g0bNviyDhEREfkJGnWIVhEREfFPCnIRERETU5CLiIiYmMcgLyoqYvHixTz88MNA\n7bbzgoICrxcmIiIinnkM8oULFxIfH8+xY8cAqKqq4tFHH/V6YSIiIuKZxyAvKChg1qxZBAUFATBh\nwgQqKiq8XpiIiIh41qht5NXV1e4jvTkcDsrKyrxalIiIiDSOx/OR33zzzUybNg273c5dd91FZmYm\njz/+uC9qExEREQ88BvmkSZNISUlh165dBAcH89RTTxEXF+eL2kRERMQDj0Gemprq/rm0tJTNmzdj\ntVrp2rUrycnJ53zs/v37ueeee7j11luZOXMmubm5zJs3D6fTic1mY+nSpQQHB7NmzRreeOMNAgIC\nmDFjBtOnT//5nYmIiFwAPAb5tm3b2LZtGykpKQQGBrJz504GDhxIVlYWI0eO5MEHH6z3cWVlZfz2\nt79lyJAh7mXPP/88N910ExMnTuRPf/oTqampTJ48meXLl5OamkpQUBDTpk1j3LhxREZGNl2XIiIi\nLZTHnd2cTicffPABL774IsuXL+df//oXrVq14r333mPHjh0NPi44OJgVK1bUmYZPS0tj7NixAIwe\nPZrt27eTkZFBUlISERERhISEkJKSQnp6ehO0JiIi0vJ5HJEfP36c2NhY9/WYmBiOHTuGxWLB5XI1\nvGKrFau17urLy8sJDg52r8dut+NwOIiOjnbfJzo6Grvdft6NiIiIXIg8BnlCQgL3338/gwYNwmKx\nsGvXLsLDw/noo4+Ij4//yb/4zNOjNmb5maKiwrBaA3/y766PzRbRpOtrTurFP7WUXlpKH6Be/FFL\n6QN814vHIF+8eDHvv/8+X3/9NS6Xi+TkZKZOncrJkycZOXLkef2ysLAwKioqCAkJ4fjx48TFxREX\nF4fD4XDfJy8vj759+55zPYWFTfs9dpstAru9pEnX2VzUi39qKb20lD5AvfijltIHNH0v5/pQ4DHI\ng4OD6+xFXlVVxcMPP8zzzz9/3oUMHTqUdevWce2117J+/XqGDx9OcnIyCxcupLi4mMDAQNLT01mw\nYMF5r1tERORC5DHI//nPf/Lss89SVFQEQEBAAIMHD/a44r1797J48WKys7OxWq2sW7eOP/zhD8yf\nP5/Vq1eTkJDA5MmTCQoKYu7cudx2221YLBbuvfdeIiJaztSKiIiIN3kM8pUrV7J27VoeeughXnrp\nJdauXduooO3duzcrV648a/nrr79+1rIJEyYwYcKERpYsIiIip3n8+llERAQ2mw2n00lYWBjXX389\n7777ri9qExEREQ88jsgDAwP57LPPiI+PZ9myZXTv3p3s7Gxf1CYiIiIeeByRL1myhPbt27NgwQLy\n8vJYs2YNv/nNb3xRm4iIiHjgcUS+ceNGrrvuOgB++9vfer0gERERaTyPI/KPP/6YkpKW8b0+ERGR\nlsbjiLyiooIxY8bQtWtXgoKC3MvfeustrxYmIiIinnkM8nvuuccXdYiIiMhP4HFqfdCgQZSVlbF/\n/34GDRpE+/btGThwoC9qExEREQ88BvnSpUtJTU3lH//4BwBr167ld7/7ndcLExEREc88BvkXX3zB\nCy+8QHh4OAD33nsv+/bt83phIiIi4pnHIG/VqhUAFosFAKfTidPp9G5VIiIi0iged3ZLSUlh/vz5\n5OXl8frrr7N+/XoGDRrki9pERETEA49B/uCDD/LRRx8RGhrK999/z69//WuuvPJKX9QmIiIiHngM\n8oceeohrr72W3/zmNwQEeJyJFxERER/ymMyjRo3i7bffZsyYMfzud78jMzPTF3WJiIhII3gckV9z\nzTVcc801lJSU8PHHH/PXv/6Vo0eP8n//93++qE9ERETOoVFz5YZh8J///IfMzEyOHDnCpZde6u26\nREREpBE8jsifeOIJNm3aRGJiIr/85S+ZN28eoaGhvqhNREREPPAY5Jdccglz5swhOjravSwnJ4eE\nhASvFiYiIiKeeQzym2++GYDKykrWrVvHu+++y6FDh9i6davXixMREZFz8xjku3fv5t133+XDDz/E\n5XLx1FNPMX78eF/UJiIiIh40uLPbihUrmDRpEg8++CAxMTG8++67dO7cmauuuqrOeclFRESk+TQ4\nIv/zn/9M9+7deeKJJxg8eDDww/HWRURExD80GOQbN27kvffe48knn8TlcjFlyhSqq6t9WZuIiIh4\n0ODUus1m44477mDdunU8/fTTHD16lOzsbO666y42bdrkyxpFRESkAY06IMzAgQN59tln2bJlC6NG\njWL58uXerktEREQa4bzOgtK6dWtuuOEG/v73v3urHhERETkPOp2ZiIiIiSnIRURETExBLiIiYmIK\nchERERNTkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiSnIRURETExBLiIiYmIKchERERNT\nkIuIiJiYglxERMTEFOQiIiImpiAXERExMQW5iIiIiVl9+cvS0tJ44IEH6NGjBwA9e/bk9ttvZ968\neTidTmw2G0uXLiU4ONiXZYmIiJiWT4McYNCgQTz//PPu64899hg33XQTEydO5E9/+hOpqancdNNN\nvi5LRETElJp9aj0tLY2xY8cCMHr0aLZv397MFYmIiJiHz0fkBw8e5K677qKoqIjZs2dTXl7unkqP\niYnBbrd7XEdUVBhWa2CT1mWzRTTp+pqTevFPLaWXltIHqBd/1FL6AN/14tMg79KlC7Nnz2bixIlk\nZWUxa9YsnE6n+3bDMBq1nsLCsiaty2aLwG4vadJ1Nhf14p9aSi8tpQ9QL/6opfQBTd/LuT4U+HRq\nvV27dkyaNAmLxULnzp2JjY2lqKiIiooKAI4fP05cXJwvSxIRETE1nwb5mjVrePXVVwGw2+3k5+cz\ndepU1q1bB8D69esZPny4L0sSERExNZ9OrY8ZM4aHH36YTz/9lOrqahYtWkRiYiKPPvooq1evJiEh\ngcmTJ/uyJBEREVPzaZC3bt2aF1988azlr7/+ui/LEBERaTGa/etnIiIi8tMpyEVERExMQS4iImJi\nCnIRERETU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERET\nU5CLiIiYmIJcRETExBTkIiIiJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiY\nmIJcRETExBTkIiIiJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETE\nxBTkIiIiJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJiCnIRERETU5CLiIiYmIJcRETExBTkIiIi\nJqYgFxERMTEFuYiIiIkpyEVERExMQS4iImJi1uYu4LSnn36ajIwMLBYLCxYsoE+fPs1dkoiIiN/z\niyD//PPP+e6771i9ejWHDh1iwYIFrF69urnLEhER8Xt+MbW+fft2rrjiCgC6detGUVERJ0+ebOaq\nRERE/J9fBLnD4SAqKsp9PTo6Grvd3owViYiImINfTK3/mGEY57w9KioMqzWwSX+nzRbRpOtrTurF\nP7WUXlpKH6Be/FFL6QN814tfBHlcXBwOh8N9PS8vD5vN1uD9CwvLmvT322wR2O0lTbrO5qJe/FNL\n6aWl9AHqxR+1lD6g6Xs514cCv5haHzZsGOvWrQNg3759xMXF0bp162auSkRExP/5xYg8JSWFXr16\nccMNN2CxWHjyySebuyQRERFT8IsgB3j44YebuwQRERHT8YupdREREflpFOQiIiImpiAXERExMb/Z\nRi4iIuJJtqOUL746ToDFQliIlfCQIPdleKiVsJAgwkOsWAMvnHGqglxEPHK6XLy78TCZR/IZcEkc\nI5ITiIpo1dxlSRMqOlnJtr3fsy0zF5fLYGhSPJcnxfvF8+x0udh9IJ8N6cf46rvCRj0mOCigNtxD\nfgh3d+DXWVb7AaCVNZCAAAuBARb35Zk/114GEBhgwWIBi8Xi5a4bT0EuIudUXFbFi//cy9dHTwCQ\nbT/C2m3f0rdHLKP7dSCxSxQBfvSm1lTKK2vIPJyPy2Vw2S/a+dUbd1NxulzsPVzA5owcMg7m4zIM\ngqwBWCzw3ubDvL/lCH26xTCibwJJF0cTGODbUW5xaRWbM3LYuDubguJKABIvimJUvw6Eh1gpq6ih\ntKKasooaTp66LK2ooayimtLy2tsKiivJtpdy7uOFnr/6At8d+hYLPTpHcfukS33yd6MgF5EGffd9\nCS/8Yw/5xZWk9LTxwI0pbEj7ls92ZZO+3076fjtxUaGM6tuBYUntiQgLbu6Sf5bi0ip2H3SQvt/O\nf74toMZZ+/b/1XeF3DL+khYzXWs/Uc6WPblsy8ylsKQ2IDu3a83I5AT3h5Yd/znO5t057D7oYPdB\nB1ERrbg8KZ7hyfHEtg31an2Hc4r5dOcxvvj6ODVOg1bBgYxJ6cDolI50iA0/7/W5XAblVTWUllef\nCvofPgCUVtQuq65x4XQZuFynLw2cp/65fnRZ92dXvbdXVNZgAL74+GcxPB3Y3A819SH8dFhA/6Re\nmte/9+byxkffUFPjYvKIi/nlkItoF9cGu70EwzA4klvCZ7uO8flXeVTXuLAGBjDwUhuj+nWge4e2\nfj+CPf2cOE6Uuz+UHMgu4vQ7Ykdba1J6xpJxMJ/vjpfwiy5R3DM5ibAQ/xv/NObvq7rGxa4DdjZn\n5PCfb2unp0NbBTL4F+0ZkZzARe3rPwTod9+XsCkjhx37vqeiyokF6NU1mhHJCfTtEdtkH26qa5x8\nnV3MPzce5EhubS/to8MY278jQ3u3J7SV//2/n4svD9GqIMecb7INUS/+yUy91Dhd/P2zg3zy5TFC\nW1m54+pfkNw9Fqi/j5Pl1fx77/ds3JXN9wW150HoaAtnVL8ODOnlf2/AhmGQbS/l6+xitu46xtG8\n2lMmW4BuHduS0sNGyiU24iJrR52VVU5eWrOP3QcddIgN54Hpfbw+Ij1f5/r7yrafZMueXP6993tO\nllcD0LNjW4YnJzDg0jhaBTXuBFSVVU4+//o4mzNyOJRdDECbsCCGJcUzIjmBdtFhP6n2/KIKNu7O\nZtPuHE6WV2MBkrvHMnZAR35xUZTffyBsiILcAwV5w9SLfzJLL8WlVbz4fu328ITYcO6bmlTnDfpc\nfRiGwddHT/DZrmx27bfjdBm0CgpkcK92jO7Xgc7tmu+sVi7D4HB2sXvknXeiHKjdzpnYJYqUnjb6\n9bDRNrz+TQMul8HfPj3AJzuP0TY8mAem96FL+za+bOGcfvy8VFTV8MVXeWze80PoRpwK3eF94omP\nOf/p6TNl20+yKSOH7Xu/p7SiBoBLO0cyIjmB/pfYCPJwdkrDMPj6u0I+Tc9m1wE7hgHhIVYmDOnC\nZZfYiI30rw9KP4WC3AMFecPUi38yQy9HcotZ/l4mBcWV9O9p479+mXjWaLqxfRSdrGTznlw2784m\n/9ROShcntGF0vw4MvDSO4EaOAn+OGqeLr78rJH2/nV0HHBSVVgHQKjiQpItjGNW/E11s4ec1Vf7x\nF1n87dMDBAUFcNc1venbI9Zb5Z8Xmy2CvLxijuSWsDkjh7SvjlN5ehr84mhG9GnaafDTqmuc7Pym\ndrr+9M6Q4SFWhvRqz4i+CXS01T35VXllDdv3fc+nO4+Rm187e3NRuwjG9O/AZYnt6JAQ6fevk8ZS\nkHugIG+YevFP/t7Ltsza7eFOp4spp7aH1zeleb59uFwGew7ns3FXNpmH8jGofaMflhTPqH4daP8T\np2Pr/A7DwOk0qHG6qK5xsT/rBOn77WQcyqe8sna02Do0iH49YknpaeMXXaIIsgb+5Odk1347L63d\nR3WNixvH9uCKAZ1+dg8/x8nyajK/K+TDbUc4Zi8FIKZNKy7vk8DlSfHEtA3xSR3HC8rYvCeHbXty\nKS6rncLvltDGvf399M51FVVOAgMsDLw0jjH9O9ItoY37b83fXyfnQ0HugYK8YerFP/lrLzVOF6s3\nHOTTnccIa2Xljmt60adbTIP3/zl9OE6Usykjhy0ZOe43+p6dImkbHkyNs3ZPYeepyxpn7d7AtZe1\nIe10uqhx1Ya2+zangauBt7CYNiGk9LSR0jOWHh0jCQio+8Hk5/RyJLeY51L3UFxaxRUDOnLDmB5n\nrd/byitr+DDtO9Z/nkVVjYvAAAv9esQyIjmBX3SJ9nk9p9U4XWQcdLApI4d9hwvqfO0rsnUwo/p1\nYGRyAm1bn/39dH99nfwUvgxy/9oLRUR8pqi0ir/+cy/7s07QITac2dcl0S7q54+QGxIbGcp1I7tx\n7eVdSd9vZ+OubPd07I+d/m5uYGDtATisgRasgQGEBAVgDQn44bZAC9ZTP5++7BAbTkpPG53btfba\njlJd49uwcFZ/nntnD598eYz8ogruuLoXrYJ9s8lg0+4c3t96hJPl1bRtHczNo3qQ3DWKNg1s4/cl\na2AA/S+Jo/8lcTiKytm6J5ccRykDE9vRzwvT+6IgF7kgHckt5oV/ZFJYUsmAS2q3h4cE++btwBoY\nwKDEdgxKbMfJ8mqcLgNroOVUYJ8+cpb/76kc2zaUx2am8Jd/7mXXAQeL/zedB6b1qXek2RQMw2Dn\nN3ZSNx0ir7CckOBApgzvypUDO9Oxg39uW45tG8rk4Rc3dxktnoJc5AKzdU8ub66r3R5+3ciLmTS4\n/u3hvtA6NKhZfm9TCQsJYs70ZN5c9w1b9+Tyuze/ZM70ZDr8aCevn2t/1gne+ewgh3KKCQywMDal\nI1cP6+IXI3BpfgpykQtEjdMviIg6AAAR/UlEQVTF3z49wIb0bMJaWbnzuiSSLm54e7g0jjUwgF9P\nvBRbZCjvbT7M06t2cs+UJHp1if7Z687NLyV14yF2HXAAMOASG9eN7PaTv7MtLZOCXOQCUFRaxV/f\ny2T/sSI62Gq/Hx7nxe3hFxqLxcLVQ7tgiwzhtX99xZ//nsGsCZcwvE/CT1pf0clK3t/2LZt35+Ay\nDLp3bMuM0d3p3qFtE1cuLYGCXKSFO5RTxF/e21u7PfzSOP5r0qU+2x5+oRn8i/ZER4Sw7N09vP7B\n19hPVDBleNdGb7qoqKph3edZfJR2lMpqJ+2jw5g+qht9e8SaYr8BaR56NYu0YFsycli5/hucLoPp\no7ox4bLOCgQv69kpksdnDeDPf8/g//79LY4T5fx6UiJB1ob31q5xutiyJ5f3tx6huLSKNuHBXD+m\nO8OT431+xjExnws+yPcezuftFWkkd49hTL8OLeLQgCLffV/Cui+OsmPfccJDrNx5bS96d9X2cF9p\nHx3G47P6s+zdTHb85zgFxRXMvq7PWTv3GYbBrgMOUjce4vuCMloFBXLt5V0ZP6iTZk2k0S74v5TQ\nVlbKKqv5KO0o6z4/St/usYzpb+6D9cuFqbLayedfHWfjrhyO5NYeX7tTXGvunZrkPgGI+E5EWDCP\n3NiXV//1FZ9/lcfv3/ySOTOS3d/VP5hdxN8/O8jBY0UEWCyM6teBa4d18drX16TluuCDvFuHtry2\n8Eo+3HqIT3ceY9cBB7sOOIiPCWNMijlPnycXlhxHKRt3Z/PvzO8pq6ypPXtUtxhG9etA0sUxzXaE\nL4EgayB3XNOL2LahfLDjO37/5k5mXtmTL77OY+c3dgD69Yhl2qhuP/tEJnLhUkIBwUGBDO0dz9De\n8adOaJ/F51/l8dbH+3l30yGGJcUztn/HJjkutEhTqK5xuY+O9k1W7dHR2oYHc1X/LoxIjve702xe\nyAIsFqaN6oYtMoSV6/bz4vv7gNrjkE8f3Z2enSKbuUIxOwX5j1yc0IaLE3oxY0wPNu/O5rNd2Xy6\n8xif7jxG767RjOnfkT4a5UgzyTtRzqbd2Wzdk0vJqeOVJ14Uxeh+HbxyditpOiP7diC2bSgff5nF\n5Unx9L/Eps130iQU5A1oGx7M1cO6MnHwRew64ODTL7PYe6SAvUcKsEWGMLpfR4YnxxMe4v9Hpiqv\nrCEnv5RcR9mpy1JKK2toHx1GQkw4CbG1l9FtQwjQG4vfcbpc7DmYz2e7s90noQgPsTJ+UCdG9m2a\nM4iJb/TqGk2vrj//QDEiZ1KQe2ANDGDgpXEMvDSOo8dL2JB+jB37jvP3zw7yz62HGdKrPWNTOtIx\nrmkPyfhTFJdV8f0hB18dcpDrKK0N7fwyCksqz7qvBTh4rKjOsuCgAOKja4M9PiachNhw4mPCiIsK\n1VdgmkFhSSWbM3LYnJHjfg67d2zL6L4dGHCpjSCr90/QISL+T0F+Hjq3i+DWiYlMG9WdrXty2ZB+\njE27c9i0O4dLOkUytn9H+vWM9WroGYZBYUklOfml5DjKyD01ws7JL+NkefVZ949u04peXaNJiAkn\n/tTIOz4mjNBWVvIKy8k5I/BzHKVkO0r57njdky9YAy20iz4V7jFhJMSGkxATTrvosHN+N1bOn8sw\n+M+RAj7blU3GwXxchkFIcCBjUjowqm8Hv/jAKCL+RUH+E7QODWLCZZ25cmAn9hzK59OdWez7tpBv\nsk4QFdGKUX0TiIsKw2UYGIaBYYDLZWBw6tIwcBmcuv30bWf8fOr2My8LSyrJza8N7MoqZ516LBaI\niwyle4e2dOsUSWRYEAmx4bSPDjvnHvcJsbWj7jO5XAaOonL3h4TTQZ+TX0a2vbTe35sQWxvqP2yf\nrT0DcX2niT5zmXH6TMV1lv3wQ2hYMOVlVe77/Xh951rXmeupW3Rt3RYstZeW2sNrnrXM/bMFy6le\nsVg4vWtEQO2d6mmw/kXh4a0oLf1hZsSo5z+nstpJ2n+OYz9RAcBF7SIYndKBQYlx+k6xiDRI7w4/\nQ0CAhb49YunbI5bc/FI27Mxm695c3ttyxCu/7/TI+PSo+oeRcah7mvXnnsw+IMBCXFQYcVFh9O0R\n617ungk4NfrPcZS6g/70CR3k5wu2BnB5n3hG9+tA1/g2zV2OiJiAgryJxMeEc/OVPZk68mJ2H3BQ\nUe0k4IzRXoDFQsDpn0+db/nHt7uXBVgI4If7WSy1B5ewRYY027Zqi8VCdJsQotuE0PuMM2YZhkFx\nWTV5hWW4XIb7vmc//tQl7h9+uO1HP5y+T1RUGCdOlJ17PfXcdmbNZ67f4IeRsGHwwyxI7ZU6y07P\npJx+jPuxBtS2WTtbUt+gvL79Bdu2DaOoqNxDvXBxfBvCTLADpYj4D4tR3xyfiIiImIL2VBIRETEx\nBbmIiIiJKchFRERMTEEuIiJiYgpyERERE1OQi4iImNgF/z3yp59+moyMDCwWCwsWLKBPnz7NXZJH\nS5YsYefOndTU1HDnnXeyYcMG9u3bR2Rk7ekQb7vtNkaNGsWaNWt44403CAgIYMaMGUyfPr2ZK68r\nLS2NBx54gB49egDQs2dPbr/9dubNm4fT6cRms7F06VKCg4P9vpd33nmHNWvWuK/v3buX3r17U1ZW\nRlhY7UlNHn30UXr37s0rr7zCRx99hMViYfbs2YwcObK5yq5j//793HPPPdx6663MnDmT3NzcRj8X\n1dXVzJ8/n5ycHAIDA3nmmWfo1KmT3/Tx2GOPUVNTg9VqZenSpdhsNnr16kVKSor7cf/zP/+Dy+Xy\nmz7q62X+/PmNfq3703NSXy/3338/hYWFAJw4cYK+ffty5513cvXVV9O7d28AoqKieP755ykpKWHu\n3LmUlJQQFhbGH//4R/f/ga/9+P03KSmp+V8nxgUsLS3NuOOOOwzDMIyDBw8aM2bMaOaKPNu+fbtx\n++23G4ZhGAUFBcbIkSONRx991NiwYUOd+5WWlhpXXnmlUVxcbJSXlxu//OUvjcLCwuYouUE7duww\n7rvvvjrL5s+fb3zwwQeGYRjGH//4R+Ott94yRS9nSktLMxYtWmTMnDnT+Oabb+rcdvToUWPKlClG\nZWWlkZ+fb4wfP96oqalppkp/UFpaasycOdNYuHChsXLlSsMwzu+5+Mc//mEsWrTIMAzD2LJli/HA\nAw/4TR/z5s0z/vWvfxmGYRirVq0yFi9ebBiGYQwaNOisx/tLH4ZRfy/n81r3917ONH/+fCMjI8PI\nysoypkyZctbty5YtM1asWGEYhmH87W9/M5YsWeL1mutT3/uvP7xOLuip9e3bt3PFFVcA0K1bN4qK\nijh58mQzV3VuAwcO5LnnngOgTZs2lJeX43Q6z7pfRkYGSUlJREREEBISQkpKCunp6b4u97ylpaUx\nduxYAEaPHs327dtN18vy5cu555576r0tLS2N4cOHExwcTHR0NB06dODgwYM+rvBswcHBrFixgri4\nOPey83kutm/fzrhx4wAYOnRosz0/9fXx5JNPMn78eKB2hHfixIkGH+8vfUD9vdTH358TOHcvhw8f\npqSk5JyzoWf2cvpvsTnU9/7rD6+TCzrIHQ4HUVFR7uvR0dHY7fZmrMizwMBA91RtamoqI0aMIDAw\nkFWrVjFr1iwefPBBCgoKcDgcREf/cN5jf+3t4MGD3HXXXdx4441s27aN8vJygoODAYiJicFut5um\nF4A9e/YQHx+PzWYD4Pnnn+fmm2/miSeeoKKiwm97sVqthISE1Fl2Ps/FmcsDAgKwWCxUVVX5roFT\n6usjLCyMwMBAnE4n//u//8vVV18NQFVVFXPnzuWGG27g9ddfB/CbPqD+XoBGv9bN0AvAm2++ycyZ\nM93XHQ4H999/PzfccIN7c9WZvcTExJCXl+f9outR3/uvP7xOLvht5GcyTHS02k8++YTU1FRee+01\n9u7dS2RkJImJibz88su88MIL9OvXr879/bG3Ll26MHv2bCZOnEhWVhazZs2qM7vQUM3+2Mtpqamp\nTJkyBYBZs2ZxySWX0LlzZ5588kneeuuts+7vz72c6XyfC3/ry+l0Mm/ePAYPHsyQIUMAmDdvHtdc\ncw0Wi4WZM2cyYMCAsx7nb31ce+21P/m17m+9QO2HqZ07d7Jo0SIAIiMjeeCBB7jmmmsoKSlh+vTp\nDB48uM5j/KGPM99/r7zySvfy5nqdXNAj8ri4OByOH87clZeX5x5J+bMtW7bw4osvsmLFCiIiIhgy\nZAiJiYkAjBkzhv3799fbm6cpOl9r164dkyZNwmKx0LlzZ2JjYykqKqKiovY0nsePHycuLs4UvZyW\nlpbmfmMdN24cnTt3Bhp+Xk736I/CwsIa/VzExcW5Zxaqq6sxDMM9SvEHjz32GBdddBGzZ892L7vx\nxhsJDw8nLCyMwYMHu58ff+7jfF7r/t4LwBdffFFnSr1169Zcd911BAUFER0dTe/evTl8+HCdXpr7\nNfPj919/eJ1c0EE+bNgw1q1bB8C+ffuIi4ujdevWzVzVuZWUlLBkyRJeeukl916b9913H1lZWUBt\nkPTo0YPk5GQyMzMpLi6mtLSU9PT0ekcczWnNmjW8+uqrANjtdvLz85k6dar7OVm/fj3Dhw83RS9Q\n+yIODw8nODgYwzC49dZbKS4uBn54XgYPHszGjRupqqri+PHj5OXl0b1792auvH5Dhw5t9HMxbNgw\nPvroIwA+++wzLrvssuYsvY41a9YQFBTE/fff7152+PBh5s6di2EY1NTUkJ6eTo8ePfy6Dzi/17q/\n9wKQmZnJpZde6r6+Y8cOnnnmGQDKysr4+uuv6dq1a51eTv8tNof63n/94XVywZ/97A9/+ANffvkl\nFouFJ598ss4flT9avXo1y5Yto2vXru5lU6dOZdWqVYSGhhIWFsYzzzxDTEwMH330Ea+++qp76vCa\na65pxsrPdvLkSR5++GGKi4uprq5m9uzZJCYm8uijj1JZWUlCQgLPPPMMQUFBft8L1H7l7M9//jOv\nvPIKAB988AGvvPIKoaGhtGvXjt///veEhoaycuVK1q5di8ViYc6cOe6p3ua0d+9eFi9eTHZ2Nlar\nlXbt2vGHP/yB+fPnN+q5cDqdLFy4kG+//Zbg4GCeffZZ4uPj/aKP/Px8WrVq5f6Q3q1bNxYtWsTS\npUvZsWMHAQEBjBkzhrvvvttv+miol5kzZ/Lyyy836rXu770sW7aMZcuW0b9/fyZNmgRATU0NCxcu\n5MiRIzidTm688Uauu+46SktLeeSRRzhx4gRt2rRh6dKlRERE+LyP+t5/n332WRYuXNisr5MLPshF\nRETM7IKeWhcRETE7BbmIiIiJKchFRERMTEEuIiJiYgpyERERE9OR3USETZs28fLLLxMQEEB5eTkd\nO3bkqaee4uDBg9hstmY9a5aInJu+fiZygauqqmL48OGsXbvWfcSspUuXEhMTw+HDh5k0aRJDhw5t\n5ipFpCEakYtc4CorKykrK6O8vNy97JFHHuHjjz/mL3/5C3v27HEf4vS///u/KS8vp6ysjIceeoih\nQ4cyf/58WrVqxbFjx8jLy2Pq1Kn8+te/bsaORC4sCnKRC1xERAT33XcfkydPJjk5mcsuu4zx48cz\nbtw43nzzTe6++26GDBnCHXfcwX/9138xePBg7HY7119/PevXrwdqD0/76quvUlxczBVXXMHkyZPr\nnFlQRLxHQS4i3HHHHUyfPp1t27aRlpbGjBkzeOihh+rcJy0tjdLSUpYvXw7UnpoyPz8fgMsvvxyo\nPUdzly5d+O677xTkIj6iIBcRysvLiYqK4qqrruKqq65iwoQJPPvss+4TQwAEBwezbNmyOudZPs3l\ncrl/NgwDi8Xik7pFRF8/E7ngbdmyheuvv56TJ0+6l2VlZXHRRRdhsViorq4GoH///nz44YcAFBQU\n8Pvf/959/7S0NACKioo4evRonZNKiIh3aa91EWHlypW8//77hIaGYhgGMTExPP7447z33nu8/fbb\nLFiwgMTERJ544gkqKyupqqri7rvvZuzYscyfPx+LxUJRURFZWVlcf/31zJw5s7lbErlgKMhF5GeZ\nP38+/fv3Z/r06c1disgFSVPrIiIiJqYRuYiIiIlpRC4iImJiCnIRERETU5CLiIiYmIJcRETExBTk\nIiIiJqYgFxERMbH/D7kErnMuHQ27AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Usj9iWTskQ3t"
      },
      "cell_type": "markdown",
      "source": [
        "# Test Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "wdZBH30Rs95B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = BeraterEnv()\n",
        "print(env.reset())\n",
        "print(env.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "oTtUfeONkQ3w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = True\n",
        "\n",
        "env = BeraterEnv()\n",
        "print(env)\n",
        "observation = env.reset()\n",
        "print(observation)\n",
        "\n",
        "for t in range(1000):\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps\".format(t+1))\n",
        "        break\n",
        "env.close()\n",
        "print(observation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eWpCU8xH0ZKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Baseline"
      ]
    },
    {
      "metadata": {
        "id": "7NxTojLi0N0o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import json\n",
        "\n",
        "class Baseline():\n",
        "\n",
        "  def __init__(self, env, verbose=1):\n",
        "    self.env = env\n",
        "    self.verbose = verbose\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.map = self.env.map\n",
        "    self.rewards = self.env.customer_reward.copy()\n",
        "    \n",
        "  def as_string(self, state):\n",
        "    # reward/cost does not hurt, but is useless, path obsucres same state\n",
        "    new_state = {\n",
        "        'rewards': state['rewards'],\n",
        "        'position': state['position']\n",
        "    }\n",
        "    return json.dumps(new_state, sort_keys=True)\n",
        "  \n",
        "  def is_goal(self, state):\n",
        "    if state['position'] != 'S': return False\n",
        "    for reward in state['rewards'].values():\n",
        "      if reward != 0: return False\n",
        "    return True\n",
        "    \n",
        "\n",
        "  def expand(self, state):\n",
        "    states = []\n",
        "    for position, cost in self.map[state['position']]:\n",
        "      new_state = deepcopy(state)\n",
        "      new_state['position'] = position\n",
        "      new_state['rewards'][position] = 0\n",
        "      reward = state['rewards'][position]\n",
        "      new_state['reward'] += reward\n",
        "      new_state['cost'] += cost\n",
        "      new_state['path'].append(position)\n",
        "      states.append(new_state)\n",
        "    return states\n",
        "\n",
        "  def search(self, root, max_depth = 25):\n",
        "      closed = set()\n",
        "      open = [root]\n",
        "\n",
        "      while open:\n",
        "          state = open.pop(0)\n",
        "          if self.as_string(state) in closed: continue  \n",
        "\n",
        "          closed.add(self.as_string(state))\n",
        "\n",
        "          depth = len(state['path'])\n",
        "          if depth > max_depth:\n",
        "            if self.verbose > 0:\n",
        "              print(\"Visited:\", len(closed))\n",
        "              print(\"Reached max depth, without reaching goal\")\n",
        "            return None\n",
        "\n",
        "          if self.is_goal(state):\n",
        "            scaled_reward = (state['reward'] - state['cost']) / 6000\n",
        "            state['scaled_reward'] = scaled_reward\n",
        "            if self.verbose > 0:\n",
        "              print(\"Scaled reward:\", scaled_reward)            \n",
        "              print(\"Perfect path\", state['path'])\n",
        "            return state\n",
        "\n",
        "          expanded = self.expand(state)\n",
        "          open += expanded\n",
        "          # make this best first\n",
        "          open.sort(key=lambda state: state['cost'])\n",
        "        \n",
        "  def find_optimum(self):\n",
        "    initial_state = {\n",
        "        'rewards': self.rewards.copy(),\n",
        "        'position': 'S',\n",
        "        'reward': 0,\n",
        "        'cost': 0,\n",
        "        'path': ['S']\n",
        "    }\n",
        "    return self.search(initial_state)\n",
        "  \n",
        "  def benchmark(self, model, sample_runs=100):\n",
        "    self.verbose = 0\n",
        "    BeraterEnv.showStep = False\n",
        "    BeraterEnv.showDone = False\n",
        "\n",
        "    perfect_rewards = []\n",
        "    model_rewards = []\n",
        "    for run in range(sample_runs):\n",
        "      observation = self.env.reset()\n",
        "      self.reset()\n",
        "      \n",
        "      optimum_state = self.find_optimum()\n",
        "      perfect_rewards.append(optimum_state['scaled_reward'])\n",
        "      \n",
        "      state = np.zeros((1, 2*128))\n",
        "      dones = np.zeros((1))\n",
        "\n",
        "      for t in range(1000):\n",
        "        actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "        observation, reward, done, info = self.env.step(actions[0])\n",
        "        if done:\n",
        "          break\n",
        "      model_rewards.append(env.totalReward)\n",
        "    return perfect_rewards, model_rewards\n",
        "  \n",
        "  def score(self, model, sample_runs=100):\n",
        "    perfect_rewards, model_rewards = self.benchmark(model, sample_runs=100)\n",
        "    \n",
        "    perfect_score_mean, perfect_score_std = np.array(perfect_rewards).mean(), np.array(perfect_rewards).std()\n",
        "    test_score_mean, test_score_std = np.array(model_rewards).mean(), np.array(model_rewards).std()\n",
        "    \n",
        "    return perfect_score_mean, perfect_score_std, test_score_mean, test_score_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "4GlYjZ3xkQ38"
      },
      "cell_type": "markdown",
      "source": [
        "# OpenAI: Train, enjoy & evaluate model"
      ]
    },
    {
      "metadata": {
        "id": "TRR6MUY9M8f4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train model"
      ]
    },
    {
      "metadata": {
        "id": "Ajxr4ahDOgHX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Estimation\n",
        "* total cost when travelling all paths (back and forth): 2500\n",
        "* all rewards: 6000\n",
        "* but: rewards are much more sparse while routes stay the same, maybe expect less\n",
        "* estimate: no illegal moves and between\n",
        "  * half the travel cost: (6000 - 1250) / 6000 = .79\n",
        "  * and full traval cost (6000 - 2500) / 6000 = 0.58\n",
        "* additionally: the agent only sees very little of the whole scenario\n",
        "  * changes with every episode\n",
        "  * was ok when network can learn fixed scenario\n"
      ]
    },
    {
      "metadata": {
        "id": "-rAaTCL0r-ql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r logs\n",
        "!mkdir logs\n",
        "!mkdir logs/berater"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LArM6BsJgUvL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NzbylmYAkQ3-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# https://github.com/openai/baselines/blob/master/baselines/deepq/experiments/train_pong.py\n",
        "# log_dir = logger.get_dir()\n",
        "log_dir = '/content/logs/berater/'\n",
        "\n",
        "import gym\n",
        "from baselines import bench\n",
        "from baselines import logger\n",
        "\n",
        "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
        "from baselines.common.vec_env.vec_monitor import VecMonitor\n",
        "from baselines.ppo2 import ppo2\n",
        "from baselines.a2c import a2c\n",
        "\n",
        "BeraterEnv.showStep = False\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "env = BeraterEnv()\n",
        "\n",
        "wrapped_env = DummyVecEnv([lambda: BeraterEnv()])\n",
        "monitored_env = VecMonitor(wrapped_env, log_dir)\n",
        "\n",
        "# https://github.com/openai/baselines/blob/master/baselines/ppo2/ppo2.py\n",
        "# https://github.com/openai/baselines/blob/master/baselines/common/models.py#L30\n",
        "# https://arxiv.org/abs/1607.06450 for layer_norm\n",
        "\n",
        "# lr linear from lr=1e-2 to lr=1e-4 (default lr=3e-4)\n",
        "def lr_range(frac):\n",
        "  # we get the remaining updates between 1 and 0\n",
        "  start_lr = 1e-2\n",
        "  end_lr = 1e-4\n",
        "  diff_lr = start_lr - end_lr\n",
        "  lr = end_lr + diff_lr * frac\n",
        "  return lr\n",
        "\n",
        "def mlp(num_layers=2, num_hidden=64, activation=tf.nn.relu, layer_norm=False):\n",
        "    def network_fn(X):\n",
        "        h = tf.layers.flatten(X)\n",
        "        for i in range(num_layers):\n",
        "            h = tf.layers.dense(h, units=num_hidden, kernel_initializer=tf.initializers.glorot_uniform(seed=17))\n",
        "            if layer_norm:\n",
        "              h = tf.contrib.layers.layer_norm(h, center=True, scale=True)\n",
        "            h = activation(h)\n",
        "        return h\n",
        "\n",
        "    return network_fn\n",
        "  \n",
        "network = mlp(num_hidden=500, num_layers=3, layer_norm=True)\n",
        "\n",
        "# Parameters\n",
        "# https://github.com/openai/baselines/blob/master/baselines/a2c/a2c.py\n",
        "model = a2c.learn(\n",
        "    env=monitored_env,\n",
        "    network=network,\n",
        "    gamma=1.0,\n",
        "    ent_coef=0.05,\n",
        "    log_interval=50000,\n",
        "    total_timesteps=1000000)\n",
        "\n",
        "\n",
        "# model.save('berater-ppo-v12.pkl')\n",
        "monitored_env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yBzvtyVcvhkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !ls -l $log_dir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0cfzto7W8Mpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing Results\n",
        "\n",
        "https://github.com/openai/baselines/blob/master/docs/viz/viz.ipynb"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TtBh4c6-kQ4K"
      },
      "cell_type": "markdown",
      "source": [
        "## Enjoy model"
      ]
    },
    {
      "metadata": {
        "id": "H_QTckfBra7l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "observation = env.reset()\n",
        "env.render()\n",
        "baseline = Baseline(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2ZWB88EVsRei",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from baselines.common import plot_util as pu\n",
        "results = pu.load_results(log_dir)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "r = results[0]\n",
        "plt.ylim(0, .75)\n",
        "# plt.plot(np.cumsum(r.monitor.l), r.monitor.r)\n",
        "plt.plot(np.cumsum(r.monitor.l), pu.smooth(r.monitor.r, radius=100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ucP0gNhhkQ4O",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state = np.zeros((1, 2*128))\n",
        "dones = np.zeros((1))\n",
        "\n",
        "BeraterEnv.showStep = True\n",
        "BeraterEnv.showDone = False\n",
        "\n",
        "for t in range(1000):\n",
        "    actions, _, state, _ = model.step(observation, S=state, M=dones)\n",
        "    observation, reward, done, info = env.step(actions[0])\n",
        "    if done:\n",
        "        print(\"Episode finished after {} timesteps, reward={}\".format(t+1, env.totalReward))\n",
        "        break\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3z35_dMMt6SW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%time baseline.find_optimum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K36GXkzyRGOO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "KMb58O_q067F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "baseline = Baseline(env)\n",
        "perfect_score_mean, perfect_score_std, test_score_mean, test_score_std = baseline.score(model, sample_runs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dr9ylHgnRIcc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# perfect scores\n",
        "perfect_score_mean, perfect_score_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rOSOoO29Rwgm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test scores for our model\n",
        "test_score_mean, test_score_std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ls8IKVV1R5SE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PDYDOEe1RcX2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}