{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190317-1_tfagents_berater-v12_vpg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbuild/tfagents/blob/master/190317_1_tfagents_berater_v12_vpg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v13\n",
        "\n",
        "## Changes from v12 (work in progress)\n",
        "* port to tfagents with reinforce (vanilla policy gradient)\n",
        "* seems to \"run\" but fails to learn\n",
        "* problem with loss function ? converges to 0 but reward stays at -3\n",
        "* openai implementation removed"
      ]
    },
    {
      "metadata": {
        "id": "PQiN7IVMS6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "berater_show_step=False \n",
        "berater_show_done=False \n",
        "berater_debug_step=False "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-agents"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-agents-nightly > /dev/null\n",
        "!pip install tf-nightly > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Environment"
      ]
    },
    {
      "metadata": {
        "id": "sQ8Nfk3MKgLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HQyb_Aq8Kg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsJ6zcXvwN53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-olom0nwiSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Berater Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "3plH2u3Swotj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        if self.debugStep:\n",
        "          pdb.set_trace()\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "        if self.stepCount >= 200:\n",
        "          if BeraterEnv.showDone:\n",
        "            print(\"Done: stepCount >= 200\")\n",
        "          done = True\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J54w2URZIme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = berater_show_step\n",
        "BeraterEnv.showDone = berater_show_done\n",
        "BeraterEnv.debugStep = berater_debug_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYaTAvAyYO-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "yhI9abUVYNrU",
        "colab_type": "code",
        "outputId": "74295e40-aee7-4490-dab7-7013bac71aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if not 'isEnvRegistered' in locals():\n",
        "  env_name=\"Berater-v1\"\n",
        "  gym.envs.registration.register(id=env_name,entry_point=BeraterEnv,max_episode_steps=1000)\n",
        "  isEnvRegistered=True\n",
        "  print(\"Berater registered as '\" + env_name + \"'\")\n",
        "else:\n",
        "  print(\"Already registered\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater registered as 'Berater-v1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX8eJGcbOJ30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vanilla Policy Gradient TF-Agent: setup, train, visualize"
      ]
    },
    {
      "metadata": {
        "id": "bzoq0VM85p46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports & Helpers"
      ]
    },
    {
      "metadata": {
        "id": "yjMTlCFssZ1P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.reinforce import reinforce_agent\n",
        "from tf_agents.drivers import dynamic_episode_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.networks import actor_distribution_network\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "098kuzbZAHg9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "  old_show_step = BeraterEnv.showStep\n",
        "  old_show_done = BeraterEnv.showDone\n",
        "  old_debug_step = BeraterEnv.debugStep\n",
        "  BeraterEnv.showStep=False\n",
        "  BeraterEnv.showDone=False\n",
        "  BeraterEnv.debugStep=False\n",
        "  \n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  BeraterEnv.showStep=old_show_step\n",
        "  BeraterEnv.showDone=old_show_done\n",
        "  BeraterEnv.debugStep=old_debug_step\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1pGZ1aRXAJto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ]
    },
    {
      "metadata": {
        "id": "ASNIg0HtsIjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name='Berater-v1'\n",
        "num_iterations=100 # @param\n",
        "fc_layers=(500,500,500,) # @param\n",
        "\n",
        "# Params for collect\n",
        "collect_episodes_per_iteration=10 # @param\n",
        "replay_buffer_capacity=2000 # @param\n",
        "# Params for train\n",
        "learning_rate=1e-3 # @param\n",
        "gradient_clipping=None\n",
        "normalize_returns=True\n",
        "# Params for eval\n",
        "num_eval_episodes=3 # @param\n",
        "eval_interval=5 # @param\n",
        "# Params for checkpoints, summaries, and logging\n",
        "log_interval=1 # @param\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eo8emDAvtH23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
        "eval_env = tf_py_environment.TFPyEnvironment(suite_gym.load(env_name))\n",
        "\n",
        "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "tf.compat.v1.set_random_seed(0)\n",
        "\n",
        "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
        "    tf_env.time_step_spec().observation,\n",
        "    tf_env.action_spec(),\n",
        "    fc_layer_params=fc_layers)\n",
        "\n",
        "tf_agent = reinforce_agent.ReinforceAgent(\n",
        "    tf_env.time_step_spec(),\n",
        "    tf_env.action_spec(),\n",
        "    actor_network=actor_net,\n",
        "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n",
        "    normalize_returns=normalize_returns,\n",
        "    gradient_clipping=gradient_clipping,\n",
        "    debug_summaries=False,\n",
        "    summarize_grads_and_vars=False)\n",
        "tf_agent.initialize()\n",
        "\n",
        "\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    tf_agent.collect_data_spec,\n",
        "    batch_size=tf_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "\n",
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy\n",
        "\n",
        "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "    tf_env,\n",
        "    collect_policy,\n",
        "    observers=[replay_buffer.add_batch],\n",
        "    num_episodes=collect_episodes_per_iteration)\n",
        "\n",
        "collect_driver.run = common.function(collect_driver.run, autograph=False)\n",
        "tf_agent.train = common.function(tf_agent.train, autograph=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DndWC9IsApT7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train"
      ]
    },
    {
      "metadata": {
        "id": "jANo00_D4SDt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep=False\n",
        "BeraterEnv.showDone=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9tm7_OYNvK_E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2129
        },
        "outputId": "68e61984-5e6b-4b91-cb5f-4cc0e7c8cdc3"
      },
      "cell_type": "code",
      "source": [
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns=[avg_return]\n",
        "loss=[]\n",
        "\n",
        "for step in range(num_iterations):\n",
        "  collect_driver.run()\n",
        "  experience = replay_buffer.gather_all()\n",
        "  total_loss = tf_agent.train(experience)\n",
        "  replay_buffer.clear()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('iteration/train_step = {}, loss = {}'.format(step, total_loss.loss.numpy()))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('iteration/train_step = {}, Average Return = {}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration/train_step = 0, loss = -214.26231384277344\n",
            "iteration/train_step = 0, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 1, loss = 6.6664605140686035\n",
            "iteration/train_step = 2, loss = -22.090158462524414\n",
            "iteration/train_step = 3, loss = -0.07825373113155365\n",
            "iteration/train_step = 4, loss = -0.08218081295490265\n",
            "iteration/train_step = 5, loss = -9.410524944541976e-05\n",
            "iteration/train_step = 5, Average Return = -2.5722203254699707\n",
            "iteration/train_step = 6, loss = 0.0\n",
            "iteration/train_step = 7, loss = 0.0021843011491000652\n",
            "iteration/train_step = 8, loss = 0.0\n",
            "iteration/train_step = 9, loss = 0.0022439472377300262\n",
            "iteration/train_step = 10, loss = -0.02581731416285038\n",
            "iteration/train_step = 10, Average Return = -3.183331251144409\n",
            "iteration/train_step = 11, loss = 0.0\n",
            "iteration/train_step = 12, loss = -6.838257104391232e-05\n",
            "iteration/train_step = 13, loss = 0.0\n",
            "iteration/train_step = 14, loss = 0.0\n",
            "iteration/train_step = 15, loss = 0.0\n",
            "iteration/train_step = 15, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 16, loss = 0.00010287266923114657\n",
            "iteration/train_step = 17, loss = 0.0\n",
            "iteration/train_step = 18, loss = 0.0\n",
            "iteration/train_step = 19, loss = 0.0\n",
            "iteration/train_step = 20, loss = 0.0\n",
            "iteration/train_step = 20, Average Return = -3.183331251144409\n",
            "iteration/train_step = 21, loss = 0.0\n",
            "iteration/train_step = 22, loss = 0.0\n",
            "iteration/train_step = 23, loss = 0.0\n",
            "iteration/train_step = 24, loss = -0.008434073068201542\n",
            "iteration/train_step = 25, loss = -5.556528890338086e-07\n",
            "iteration/train_step = 25, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 26, loss = 0.0\n",
            "iteration/train_step = 27, loss = 9.292996764997952e-06\n",
            "iteration/train_step = 28, loss = 0.016645394265651703\n",
            "iteration/train_step = 29, loss = 0.0\n",
            "iteration/train_step = 30, loss = -5.621375748887658e-05\n",
            "iteration/train_step = 30, Average Return = -3.3499975204467773\n",
            "iteration/train_step = 31, loss = 0.0\n",
            "iteration/train_step = 32, loss = 0.0\n",
            "iteration/train_step = 33, loss = -18.48151969909668\n",
            "iteration/train_step = 34, loss = -4.7639546394348145\n",
            "iteration/train_step = 35, loss = -0.07789360731840134\n",
            "iteration/train_step = 35, Average Return = -3.2944419384002686\n",
            "iteration/train_step = 36, loss = 7.1518449783325195\n",
            "iteration/train_step = 37, loss = -0.006173841655254364\n",
            "iteration/train_step = 38, loss = 0.0\n",
            "iteration/train_step = 39, loss = 0.0\n",
            "iteration/train_step = 40, loss = 0.0\n",
            "iteration/train_step = 40, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 41, loss = 0.0\n",
            "iteration/train_step = 42, loss = -0.42209744453430176\n",
            "iteration/train_step = 43, loss = -3.0983595934230834e-05\n",
            "iteration/train_step = 44, loss = 0.0\n",
            "iteration/train_step = 45, loss = 0.0\n",
            "iteration/train_step = 45, Average Return = -3.183331251144409\n",
            "iteration/train_step = 46, loss = 0.0\n",
            "iteration/train_step = 47, loss = 0.0\n",
            "iteration/train_step = 48, loss = 0.0\n",
            "iteration/train_step = 49, loss = 0.0\n",
            "iteration/train_step = 50, loss = 0.0\n",
            "iteration/train_step = 50, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 51, loss = 0.0\n",
            "iteration/train_step = 52, loss = 0.0\n",
            "iteration/train_step = 53, loss = 0.0\n",
            "iteration/train_step = 54, loss = 0.0\n",
            "iteration/train_step = 55, loss = 0.0\n",
            "iteration/train_step = 55, Average Return = -3.238886594772339\n",
            "iteration/train_step = 56, loss = 0.0\n",
            "iteration/train_step = 57, loss = 0.0\n",
            "iteration/train_step = 58, loss = 0.0\n",
            "iteration/train_step = 59, loss = 0.0\n",
            "iteration/train_step = 60, loss = 0.0\n",
            "iteration/train_step = 60, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 61, loss = 0.0\n",
            "iteration/train_step = 62, loss = 0.0\n",
            "iteration/train_step = 63, loss = 0.0\n",
            "iteration/train_step = 64, loss = 0.0\n",
            "iteration/train_step = 65, loss = 0.0\n",
            "iteration/train_step = 65, Average Return = -3.238886594772339\n",
            "iteration/train_step = 66, loss = 0.0\n",
            "iteration/train_step = 67, loss = 0.0\n",
            "iteration/train_step = 68, loss = 0.0\n",
            "iteration/train_step = 69, loss = 0.0\n",
            "iteration/train_step = 70, loss = 0.0\n",
            "iteration/train_step = 70, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 71, loss = 0.0\n",
            "iteration/train_step = 72, loss = 0.0\n",
            "iteration/train_step = 73, loss = 0.0\n",
            "iteration/train_step = 74, loss = 0.0\n",
            "iteration/train_step = 75, loss = 0.0\n",
            "iteration/train_step = 75, Average Return = -3.1277754306793213\n",
            "iteration/train_step = 76, loss = 0.0\n",
            "iteration/train_step = 77, loss = 0.0\n",
            "iteration/train_step = 78, loss = 0.0\n",
            "iteration/train_step = 79, loss = 0.0\n",
            "iteration/train_step = 80, loss = 0.0\n",
            "iteration/train_step = 80, Average Return = -3.3499975204467773\n",
            "iteration/train_step = 81, loss = 0.0\n",
            "iteration/train_step = 82, loss = 0.0\n",
            "iteration/train_step = 83, loss = 0.0\n",
            "iteration/train_step = 84, loss = 0.0\n",
            "iteration/train_step = 85, loss = 0.0\n",
            "iteration/train_step = 85, Average Return = -3.3499975204467773\n",
            "iteration/train_step = 86, loss = 0.0\n",
            "iteration/train_step = 87, loss = 0.0\n",
            "iteration/train_step = 88, loss = 0.0\n",
            "iteration/train_step = 89, loss = 0.0\n",
            "iteration/train_step = 90, loss = 0.0\n",
            "iteration/train_step = 90, Average Return = -3.183331251144409\n",
            "iteration/train_step = 91, loss = 0.0\n",
            "iteration/train_step = 92, loss = 0.0\n",
            "iteration/train_step = 93, loss = 0.0\n",
            "iteration/train_step = 94, loss = 0.0\n",
            "iteration/train_step = 95, loss = 0.0\n",
            "iteration/train_step = 95, Average Return = -3.183331251144409\n",
            "iteration/train_step = 96, loss = 0.0\n",
            "iteration/train_step = 97, loss = 0.0\n",
            "iteration/train_step = 98, loss = 0.0\n",
            "iteration/train_step = 99, loss = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vMDJZNG3akLF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualize"
      ]
    },
    {
      "metadata": {
        "id": "mTI4jEGaVTbW",
        "colab_type": "code",
        "outputId": "decbd064-ace7-447b-d561-bc346770e7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps = range(0, len(returns)*eval_interval, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=1,bottom=-5)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAFcCAYAAADlIuYrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxP8DUD/kn69t7SsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}