{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190316-1_tfagents_berater-v12_dqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "w3OdHyWEEEwy"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbuild/tfagents/blob/master/190316_1_tfagents_berater_v12_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v13\n",
        "\n",
        "## Changes from v12 (work in progress)\n",
        "* port to tfagents with dqn\n",
        "* openai implementation removed"
      ]
    },
    {
      "metadata": {
        "id": "PQiN7IVMS6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "berater_show_step=False # @param\n",
        "berater_show_done=False # @param\n",
        "berater_debug_step=False # @param\n",
        "\n",
        "num_iterations = 10000  # @param\n",
        "\n",
        "initial_collect_steps = 1000  # @param\n",
        "collect_steps_per_iteration = 1000  # @param\n",
        "replay_buffer_capacity = 100000  # @param\n",
        "\n",
        "fc_layer_params = (500,)\n",
        "\n",
        "batch_size = 64  # @param\n",
        "learning_rate = 1e-3  # @param\n",
        "log_interval = 10  # @param\n",
        "\n",
        "num_eval_episodes = 10  # @param\n",
        "eval_interval = 50  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-agents"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-agents-nightly > /dev/null\n",
        "!pip install tf-nightly > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Environment"
      ]
    },
    {
      "metadata": {
        "id": "sQ8Nfk3MKgLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HQyb_Aq8Kg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n",
        "\n",
        "import pdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsJ6zcXvwN53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-olom0nwiSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Berater Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "3plH2u3Swotj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        if self.debugStep:\n",
        "          pdb.set_trace()\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "        if self.stepCount >= 200:\n",
        "          if BeraterEnv.showDone:\n",
        "            print(\"Done: stepCount >= 200\")\n",
        "          done = True\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J54w2URZIme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = berater_show_step\n",
        "BeraterEnv.showDone = berater_show_done\n",
        "BeraterEnv.debugStep = berater_debug_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYaTAvAyYO-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "yhI9abUVYNrU",
        "colab_type": "code",
        "outputId": "1c11f89f-4296-49e3-e5ed-71731af6fdf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "if not 'isEnvRegistered' in locals():\n",
        "  env_id=\"Berater-v1\"\n",
        "  gym.envs.registration.register(id=env_id,entry_point=BeraterEnv,max_episode_steps=1000)\n",
        "  isEnvRegistered=True\n",
        "  print(\"Berater registered as '\" + env_id + \"'\")\n",
        "else:\n",
        "  print(\"Already registered\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater registered as 'Berater-v1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX8eJGcbOJ30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF-Agents: Train, enjoy, evaluate"
      ]
    },
    {
      "metadata": {
        "id": "bzoq0VM85p46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports & Helpers"
      ]
    },
    {
      "metadata": {
        "id": "5ofYknQFRkRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import trajectory\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiP6UgA65163",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "  old_show_step = BeraterEnv.showStep\n",
        "  old_show_done = BeraterEnv.showDone\n",
        "  old_debug_step = BeraterEnv.debugStep\n",
        "  BeraterEnv.showStep=False\n",
        "  BeraterEnv.showDone=False\n",
        "  BeraterEnv.debugStep=False\n",
        "  \n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  BeraterEnv.showStep=old_show_step\n",
        "  BeraterEnv.showDone=old_show_done\n",
        "  BeraterEnv.debugStep=old_debug_step\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZoGMeUBr2tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup tf-agent envs, models, policies & agents"
      ]
    },
    {
      "metadata": {
        "id": "sldmm0LmcINW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name=\"Berater-v1\"\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYOUS69Mh3tj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FuiFfYjh9-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.compat.v2.Variable(0)\n",
        "\n",
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "tf_agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hss9CFs2iEt9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CNLGWERiStH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#avg_rnd_return = compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "#print(\"avgerage return with random policy={}\".format(avg_rnd_return))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SVcuWB_FsHa8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup replay buffer (train_env)"
      ]
    },
    {
      "metadata": {
        "id": "dYdP4HYHrgGq",
        "colab_type": "code",
        "outputId": "97fd0448-4909-401d-8661-9703c4283680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "print(\"replay buffer batch_size=\" + str(train_env.batch_size) + \\\n",
        "      \" max_length=\" + str(replay_buffer_capacity) + \\\n",
        "      \" size=\" + str(replay_buffer.gather_all().observation.shape[1])\n",
        "     )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay buffer batch_size=1 max_length=100000 size=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UD3yzROJrwUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "  # print(\"collect_step: trajectory=\" + str(traj))\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  replay_buffer.add_batch(traj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDSOinfuVk7u",
        "colab_type": "code",
        "outputId": "fd76415d-7841-4ea0-ee31-b0f6634e4aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay_buffer size=0 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HyrMbdf1kNXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = False # @param\n",
        "BeraterEnv.showDone = False # @param\n",
        "BeraterEnv.debugStep = False # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZDRID-p0QOKo",
        "colab_type": "code",
        "outputId": "e59a596e-83fa-4d37-e4a8-33d3a29c8d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"initial_collect_steps=\" + str(initial_collect_steps))\n",
        "for _ in range(initial_collect_steps):\n",
        "  collect_step(train_env, random_policy)\n",
        "\n",
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_collect_steps=1000\n",
            "replay_buffer size=1000 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bCHhtAfPsk9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwMsD8ATcBBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(iterator.next()[0].action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myOp6C8ks0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the agent"
      ]
    },
    {
      "metadata": {
        "id": "YnptN6_dNJBL",
        "colab_type": "code",
        "outputId": "b669992f-c953-4cae-ebc8-a7c8552b1546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "print(\"retuns={} (before training) num_eval_episodes={}\".format(returns,num_eval_episodes))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "retuns=[-33.499966] (before training) num_eval_episodes=10\n",
            "CPU times: user 7.09 s, sys: 216 ms, total: 7.31 s\n",
            "Wall time: 7.16 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qhF4Ve5NitDD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = False # @param\n",
        "BeraterEnv.showDone = True # @param\n",
        "BeraterEnv.debugStep = False # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eu-ujS7tmS5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Ui4l-FVIsyxp",
        "colab_type": "code",
        "outputId": "a5147676-e6f0-4c6f-b30f-504bc48a2937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5467
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect one step using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, tf_agent.collect_policy)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = tf_agent.train(experience)\n",
        "\n",
        "  step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('iteration/train_step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('iteration/train_step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done: stepCount >= 200\n",
            "Done: episodes=     8  avgSteps=127.75  avgTotalReward=-7.89\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=     9  avgSteps=135.89  avgTotalReward=-10.52\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    10  avgSteps=142.40  avgTotalReward=-12.58\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    11  avgSteps=147.73  avgTotalReward=-14.11\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    12  avgSteps=152.17  avgTotalReward=-15.17\n",
            "step = 1: loss = 4766.3837890625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    13  avgSteps=155.92  avgTotalReward=-16.37\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    14  avgSteps=159.14  avgTotalReward=-17.34\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    15  avgSteps=161.93  avgTotalReward=-18.24\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    16  avgSteps=164.38  avgTotalReward=-18.18\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    17  avgSteps=166.53  avgTotalReward=-18.96\n",
            "step = 2: loss = 4624.9423828125\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    18  avgSteps=168.44  avgTotalReward=-18.87\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    19  avgSteps=170.16  avgTotalReward=-18.17\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    20  avgSteps=171.70  avgTotalReward=-18.78\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    21  avgSteps=173.10  avgTotalReward=-19.22\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    22  avgSteps=174.36  avgTotalReward=-18.93\n",
            "step = 3: loss = 3504.48779296875\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    23  avgSteps=175.52  avgTotalReward=-19.43\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    24  avgSteps=176.58  avgTotalReward=-19.78\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    25  avgSteps=177.56  avgTotalReward=-19.47\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    26  avgSteps=178.46  avgTotalReward=-19.15\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    27  avgSteps=179.30  avgTotalReward=-19.25\n",
            "step = 4: loss = 3966.416748046875\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    28  avgSteps=180.07  avgTotalReward=-19.66\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    29  avgSteps=180.79  avgTotalReward=-19.67\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    30  avgSteps=181.47  avgTotalReward=-19.83\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    31  avgSteps=182.10  avgTotalReward=-20.18\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    32  avgSteps=182.69  avgTotalReward=-20.49\n",
            "step = 5: loss = 2668.2861328125\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    33  avgSteps=183.24  avgTotalReward=-20.80\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    34  avgSteps=183.76  avgTotalReward=-20.52\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    35  avgSteps=184.26  avgTotalReward=-20.23\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    36  avgSteps=184.72  avgTotalReward=-20.51\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    37  avgSteps=185.16  avgTotalReward=-20.60\n",
            "step = 6: loss = 2873.987548828125\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    38  avgSteps=185.58  avgTotalReward=-20.84\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    39  avgSteps=185.97  avgTotalReward=-20.62\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    40  avgSteps=186.35  avgTotalReward=-20.43\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    41  avgSteps=186.71  avgTotalReward=-20.39\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    42  avgSteps=187.05  avgTotalReward=-20.63\n",
            "step = 7: loss = 2923.1572265625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    43  avgSteps=187.37  avgTotalReward=-20.36\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    44  avgSteps=187.68  avgTotalReward=-20.12\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    45  avgSteps=187.98  avgTotalReward=-20.04\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    46  avgSteps=188.26  avgTotalReward=-19.78\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    47  avgSteps=188.53  avgTotalReward=-19.84\n",
            "step = 8: loss = 1310.0826416015625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    48  avgSteps=188.79  avgTotalReward=-19.85\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    49  avgSteps=189.04  avgTotalReward=-20.05\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    50  avgSteps=189.28  avgTotalReward=-20.00\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    51  avgSteps=189.51  avgTotalReward=-19.77\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    52  avgSteps=189.73  avgTotalReward=-19.55\n",
            "step = 9: loss = 3673.6337890625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    53  avgSteps=189.94  avgTotalReward=-19.69\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    54  avgSteps=190.15  avgTotalReward=-19.52\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    55  avgSteps=190.35  avgTotalReward=-19.62\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    56  avgSteps=190.54  avgTotalReward=-19.80\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    57  avgSteps=190.72  avgTotalReward=-19.60\n",
            "step = 10: loss = 2722.91015625\n",
            "step = 10: Average Return = -24.910808563232422\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    58  avgSteps=190.90  avgTotalReward=-19.45\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    59  avgSteps=191.07  avgTotalReward=-19.27\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    60  avgSteps=191.23  avgTotalReward=-19.10\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    61  avgSteps=191.39  avgTotalReward=-19.09\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    62  avgSteps=191.55  avgTotalReward=-19.16\n",
            "step = 11: loss = 1748.888916015625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    63  avgSteps=191.70  avgTotalReward=-19.31\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    64  avgSteps=191.84  avgTotalReward=-19.47\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    65  avgSteps=191.98  avgTotalReward=-19.64\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    66  avgSteps=192.12  avgTotalReward=-19.81\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    67  avgSteps=192.25  avgTotalReward=-19.74\n",
            "step = 12: loss = 2120.582763671875\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    68  avgSteps=192.38  avgTotalReward=-19.63\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    69  avgSteps=192.51  avgTotalReward=-19.78\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    70  avgSteps=192.63  avgTotalReward=-19.94\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    71  avgSteps=192.75  avgTotalReward=-19.99\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    72  avgSteps=192.86  avgTotalReward=-20.14\n",
            "step = 13: loss = 2274.793212890625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    73  avgSteps=192.97  avgTotalReward=-20.28\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    74  avgSteps=193.08  avgTotalReward=-20.41\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    75  avgSteps=193.19  avgTotalReward=-20.31\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    76  avgSteps=193.29  avgTotalReward=-20.44\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    77  avgSteps=193.39  avgTotalReward=-20.57\n",
            "step = 14: loss = 1306.0244140625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    78  avgSteps=193.49  avgTotalReward=-20.71\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    79  avgSteps=193.58  avgTotalReward=-20.78\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    80  avgSteps=193.68  avgTotalReward=-20.88\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    81  avgSteps=193.77  avgTotalReward=-20.79\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    82  avgSteps=193.85  avgTotalReward=-20.91\n",
            "step = 15: loss = 3572.7412109375\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    83  avgSteps=193.94  avgTotalReward=-21.02\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    84  avgSteps=194.02  avgTotalReward=-21.13\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    85  avgSteps=194.11  avgTotalReward=-21.25\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    86  avgSteps=194.19  avgTotalReward=-21.36\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    87  avgSteps=194.26  avgTotalReward=-21.47\n",
            "step = 16: loss = 3445.755859375\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    88  avgSteps=194.34  avgTotalReward=-21.59\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    89  avgSteps=194.42  avgTotalReward=-21.67\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    90  avgSteps=194.49  avgTotalReward=-21.76\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    91  avgSteps=194.56  avgTotalReward=-21.70\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    92  avgSteps=194.63  avgTotalReward=-21.74\n",
            "step = 17: loss = 2709.353759765625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    93  avgSteps=194.70  avgTotalReward=-21.83\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    94  avgSteps=194.77  avgTotalReward=-21.91\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    95  avgSteps=194.83  avgTotalReward=-22.00\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    96  avgSteps=194.90  avgTotalReward=-22.07\n",
            "step = 18: loss = 2493.92041015625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    97  avgSteps=194.96  avgTotalReward=-22.15\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    98  avgSteps=195.02  avgTotalReward=-22.24\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=    99  avgSteps=195.08  avgTotalReward=-22.24\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   100  avgSteps=195.14  avgTotalReward=-22.31\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   101  avgSteps=201.00  avgTotalReward=-30.77\n",
            "step = 19: loss = 4791.744140625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   102  avgSteps=201.00  avgTotalReward=-29.32\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   103  avgSteps=201.00  avgTotalReward=-27.29\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   104  avgSteps=201.00  avgTotalReward=-28.44\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   105  avgSteps=201.00  avgTotalReward=-28.95\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   106  avgSteps=201.00  avgTotalReward=-29.33\n",
            "step = 20: loss = 6423.4970703125\n",
            "step = 20: Average Return = -30.484970092773438\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   107  avgSteps=201.00  avgTotalReward=-29.31\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   108  avgSteps=201.00  avgTotalReward=-29.27\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   109  avgSteps=201.00  avgTotalReward=-29.35\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   110  avgSteps=201.00  avgTotalReward=-28.43\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   111  avgSteps=201.00  avgTotalReward=-28.74\n",
            "step = 21: loss = 4974.5087890625\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   112  avgSteps=201.00  avgTotalReward=-28.88\n",
            "Done: stepCount >= 200\n",
            "Done: episodes=   113  avgSteps=201.00  avgTotalReward=-29.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9cce37ca5a83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nfor _ in range(num_iterations):\\n\\n  # Collect one step using collect_policy and save to the replay buffer.\\n  for _ in range(collect_steps_per_iteration):\\n    collect_step(train_env, tf_agent.collect_policy)\\n\\n  # Sample a batch of data from the buffer and update the agent's network.\\n  experience, unused_info = next(iterator)\\n  train_loss = tf_agent.train(experience)\\n\\n  step = tf_agent.train_step_counter.numpy()\\n\\n  if step % log_interval == 0:\\n    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\\n\\n  if step % eval_interval == 0:\\n    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\\n    print('step = {0}: Average Return = {1}'.format(step, avg_return))\\n    returns.append(avg_return)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-28929ea9f27b>\u001b[0m in \u001b[0;36mcollect_step\u001b[0;34m(environment, policy)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Add trajectory to the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/replay_buffers/replay_buffer.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0mAdds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreplay\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   def get_next(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py\u001b[0m in \u001b[0;36m_add_batch\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m       \u001b[0mid_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_increment_last_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m       \u001b[0mwrite_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_for_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m       \u001b[0mwrite_id_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py\u001b[0m in \u001b[0;36m_increment_last_id\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_assign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_id_cs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_assign_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_last_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/critical_section_ops.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, fn, exclusive_resource_access, name)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# Ensure that mutex locking only happens *after* all args and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# kwargs have been executed.  This avoids certain types of deadlocks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mlock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_resource_variable_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmutex_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mmutex_lock\u001b[0;34m(mutex, name)\u001b[0m\n\u001b[1;32m    414\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m    415\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \"MutexLock\", name, _ctx._post_execution_callbacks, mutex)\n\u001b[0m\u001b[1;32m    417\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ty7ul0Lk6nou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training rewards"
      ]
    },
    {
      "metadata": {
        "id": "6vrAtGt3ck4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"num_iterations={} eval_interval={} tf_agent: train_step_counter={} train_sequence_length={}\".format(\\\n",
        "                 num_iterations, \\\n",
        "                 eval_interval, \\\n",
        "                 tf_agent.train_step_counter.numpy(), \\\n",
        "                 tf_agent.train_sequence_length))\n",
        "print(\"returns={}\".format(returns))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58gwqW92M_d_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}