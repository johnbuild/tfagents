{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "190316-1_tfagents_berater-v12_dqn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "zpzHtN3-kQ26",
        "bzoq0VM85p46",
        "NZoGMeUBr2tx",
        "SVcuWB_FsHa8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnbuild/tfagents/blob/master/190316_1_tfagents_berater_v12_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eU7ylMh1kQ2y"
      },
      "cell_type": "markdown",
      "source": [
        "# Berater Environment v13\n",
        "\n",
        "## Changes from v12 (work in progress)\n",
        "* port to tfagents with dqn\n",
        "* openai implementation removed"
      ]
    },
    {
      "metadata": {
        "id": "PQiN7IVMS6SC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "berater_show_step=False # @param\n",
        "berater_show_done=False # @param\n",
        "\n",
        "num_iterations = 20000  # @param\n",
        "\n",
        "initial_collect_steps = 1000  # @param\n",
        "collect_steps_per_iteration = 100  # @param\n",
        "replay_buffer_capacity = 100000  # @param\n",
        "\n",
        "fc_layer_params = (100,)\n",
        "\n",
        "batch_size = 64  # @param\n",
        "learning_rate = 1e-3  # @param\n",
        "log_interval = 10  # @param\n",
        "\n",
        "num_eval_episodes = 10  # @param\n",
        "eval_interval = 20  # @param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpzHtN3-kQ26"
      },
      "cell_type": "markdown",
      "source": [
        "## Install tf-agents"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0E567zPTkQ28",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-agents-nightly > /dev/null\n",
        "!pip install tf-nightly > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w3OdHyWEEEwy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Environment"
      ]
    },
    {
      "metadata": {
        "id": "sQ8Nfk3MKgLt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HQyb_Aq8Kg9j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import gym\n",
        "from gym.utils import seeding\n",
        "from gym import spaces\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OsJ6zcXvwN53",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper methods"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-S4sZG5ZkQ3T",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def state_name_to_int(state):\n",
        "    state_name_map = {\n",
        "        'S': 0,\n",
        "        'A': 1,\n",
        "        'B': 2,\n",
        "        'C': 3,\n",
        "        'D': 4,\n",
        "        'E': 5,\n",
        "        'F': 6,\n",
        "        'G': 7,\n",
        "        'H': 8,\n",
        "        'K': 9,\n",
        "        'L': 10,\n",
        "        'M': 11,\n",
        "        'N': 12,\n",
        "        'O': 13\n",
        "    }\n",
        "    return state_name_map[state]\n",
        "\n",
        "def int_to_state_name(state_as_int):\n",
        "    state_map = {\n",
        "        0: 'S',\n",
        "        1: 'A',\n",
        "        2: 'B',\n",
        "        3: 'C',\n",
        "        4: 'D',\n",
        "        5: 'E',\n",
        "        6: 'F',\n",
        "        7: 'G',\n",
        "        8: 'H',\n",
        "        9: 'K',\n",
        "        10: 'L',\n",
        "        11: 'M',\n",
        "        12: 'N',\n",
        "        13: 'O'\n",
        "    }\n",
        "    return state_map[state_as_int]\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x-olom0nwiSX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Berater Environment (OpenAI Gym)"
      ]
    },
    {
      "metadata": {
        "id": "3plH2u3Swotj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BeraterEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    The Berater Problem\n",
        "\n",
        "    Actions: \n",
        "    There are 4 discrete deterministic actions, each choosing one direction\n",
        "    \"\"\"\n",
        "    metadata = {'render.modes': ['ansi']}\n",
        "    \n",
        "    showStep = False\n",
        "    showDone = True\n",
        "    envEpisodeModulo = 100\n",
        "\n",
        "    def __init__(self):\n",
        "#         self.map = {\n",
        "#             'S': [('A', 100), ('B', 400), ('C', 200 )],\n",
        "#             'A': [('B', 250), ('C', 400), ('S', 100 )],\n",
        "#             'B': [('A', 250), ('C', 250), ('S', 400 )],\n",
        "#             'C': [('A', 400), ('B', 250), ('S', 200 )]\n",
        "#         }\n",
        "        self.map = {\n",
        "            'S': [('A', 300), ('B', 100), ('C', 200 )],\n",
        "            'A': [('S', 300), ('B', 100), ('E', 100 ), ('D', 100 )],\n",
        "            'B': [('S', 100), ('A', 100), ('C', 50 ), ('K', 200 )],\n",
        "            'C': [('S', 200), ('B', 50), ('M', 100 ), ('L', 200 )],\n",
        "            'D': [('A', 100), ('F', 50)],\n",
        "            'E': [('A', 100), ('F', 100), ('H', 100)],\n",
        "            'F': [('D', 50), ('E', 100), ('G', 200)],\n",
        "            'G': [('F', 200), ('O', 300)],\n",
        "            'H': [('E', 100), ('K', 300)],\n",
        "            'K': [('B', 200), ('H', 300)],\n",
        "            'L': [('C', 200), ('M', 50)],\n",
        "            'M': [('C', 100), ('L', 50), ('N', 100)],\n",
        "            'N': [('M', 100), ('O', 100)],\n",
        "            'O': [('N', 100), ('G', 300)]\n",
        "        }\n",
        "        max_paths = 4\n",
        "        self.action_space = spaces.Discrete(max_paths)\n",
        "      \n",
        "        positions = len(self.map)\n",
        "        # observations: position, reward of all 4 local paths, rest reward of all locations\n",
        "        # non existing path is -1000 and no position change\n",
        "        # look at what #getObservation returns if you are confused\n",
        "        low = np.append(np.append([0], np.full(max_paths, -1000)), np.full(positions, 0))\n",
        "        high = np.append(np.append([positions - 1], np.full(max_paths, 1000)), np.full(positions, 1000))\n",
        "        self.observation_space = spaces.Box(low=low,\n",
        "                                             high=high,\n",
        "                                             dtype=np.float32)\n",
        "        self.reward_range = (-1, 1)\n",
        "\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.envReward = 0\n",
        "        self.envEpisodeCount = 0\n",
        "        self.envStepCount = 0\n",
        "\n",
        "        self.reset()\n",
        "        self.optimum = self.calculate_customers_reward()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def iterate_path(self, state, action):\n",
        "        paths = self.map[state]\n",
        "        if action < len(paths):\n",
        "          return paths[action]\n",
        "        else:\n",
        "          # sorry, no such action, stay where you are and pay a high penalty\n",
        "          return (state, 1000)\n",
        "      \n",
        "    def step(self, action):\n",
        "        destination, cost = self.iterate_path(self.state, action)\n",
        "        lastState = self.state\n",
        "        customerReward = self.customer_reward[destination]\n",
        "        reward = (customerReward - cost) / self.optimum\n",
        "\n",
        "        self.state = destination\n",
        "        self.customer_visited(destination)\n",
        "        done = (destination == 'S' and self.all_customers_visited())\n",
        "        if self.stepCount >= 200:\n",
        "          if BeraterEnv.showDone:\n",
        "            print(\"Done: stepCount >= 200\")\n",
        "          done = True\n",
        "\n",
        "        stateAsInt = state_name_to_int(self.state)\n",
        "        self.totalReward += reward\n",
        "        self.stepCount += 1\n",
        "        self.envReward += reward\n",
        "        self.envStepCount += 1\n",
        "\n",
        "        if self.showStep:\n",
        "            print( \"Episode: \" + (\"%4.0f  \" % self.envEpisodeCount) + \n",
        "                   \" Step: \" + (\"%4.0f  \" % self.stepCount) + \n",
        "                   lastState + ' --' + str(action) + '-> ' + self.state + \n",
        "                   ' R=' + (\"% 2.2f\" % reward) + ' totalR=' + (\"% 3.2f\" % self.totalReward) + \n",
        "                   ' cost=' + (\"%4.0f\" % cost) + ' customerR=' + (\"%4.0f\" % customerReward) + ' optimum=' + (\"%4.0f\" % self.optimum)      \n",
        "                   )\n",
        "\n",
        "        if done and not self.isDone:\n",
        "            self.envEpisodeCount += 1\n",
        "            if BeraterEnv.showDone:\n",
        "                episodes = BeraterEnv.envEpisodeModulo\n",
        "                if (self.envEpisodeCount % BeraterEnv.envEpisodeModulo != 0):\n",
        "                    episodes = self.envEpisodeCount % BeraterEnv.envEpisodeModulo\n",
        "                print( \"Done: \" + \n",
        "                        (\"episodes=%6.0f  \" % self.envEpisodeCount) + \n",
        "                        (\"avgSteps=%6.2f  \" % (self.envStepCount/episodes)) + \n",
        "                        (\"avgTotalReward=% 3.2f\" % (self.envReward/episodes) )\n",
        "                        )\n",
        "                if (self.envEpisodeCount%BeraterEnv.envEpisodeModulo) == 0:\n",
        "                    self.envReward = 0\n",
        "                    self.envStepCount = 0\n",
        "\n",
        "        self.isDone = done\n",
        "        observation = self.getObservation(stateAsInt)\n",
        "        info = {\"from\": self.state, \"to\": destination}\n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def getObservation(self, position):\n",
        "        result = np.array([ position, \n",
        "                               self.getPathObservation(position, 0),\n",
        "                               self.getPathObservation(position, 1),\n",
        "                               self.getPathObservation(position, 2),\n",
        "                               self.getPathObservation(position, 3)\n",
        "                              ],\n",
        "                             dtype=np.float32)\n",
        "        all_rest_rewards = list(self.customer_reward.values())\n",
        "        result = np.append(result, all_rest_rewards)\n",
        "        return result\n",
        "\n",
        "    def getPathObservation(self, position, path):\n",
        "        source = int_to_state_name(position)\n",
        "        paths = self.map[self.state]\n",
        "        if path < len(paths):\n",
        "          target, cost = paths[path]\n",
        "          reward = self.customer_reward[target] \n",
        "          result = reward - cost\n",
        "        else:\n",
        "          result = -1000\n",
        "\n",
        "        return result\n",
        "\n",
        "    def customer_visited(self, customer):\n",
        "        self.customer_reward[customer] = 0\n",
        "\n",
        "    def all_customers_visited(self):\n",
        "        return self.calculate_customers_reward() == 0\n",
        "\n",
        "    def calculate_customers_reward(self):\n",
        "        sum = 0\n",
        "        for value in self.customer_reward.values():\n",
        "            sum += value\n",
        "        return sum\n",
        "\n",
        "      \n",
        "    def modulate_reward(self):\n",
        "      number_of_customers = len(self.map) - 1\n",
        "      number_per_consultant = int(number_of_customers/2)\n",
        "#       number_per_consultant = int(number_of_customers/1.5)\n",
        "      self.customer_reward = {\n",
        "          'S': 0\n",
        "      }\n",
        "      for customer_nr in range(1, number_of_customers + 1):\n",
        "        self.customer_reward[int_to_state_name(customer_nr)] = 0\n",
        "      \n",
        "      # every consultant only visits a few random customers\n",
        "      samples = random.sample(range(1, number_of_customers + 1), k=number_per_consultant)\n",
        "      key_list = list(self.customer_reward.keys())\n",
        "      for sample in samples:\n",
        "        self.customer_reward[key_list[sample]] = 1000\n",
        "\n",
        "      \n",
        "    def reset(self):\n",
        "        self.totalReward = 0\n",
        "        self.stepCount = 0\n",
        "        self.isDone = False\n",
        "\n",
        "        self.modulate_reward()\n",
        "        self.state = 'S'\n",
        "        return self.getObservation(state_name_to_int(self.state))\n",
        "      \n",
        "    def render(self):\n",
        "      print(self.customer_reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9J54w2URZIme",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BeraterEnv.showStep = berater_show_step\n",
        "BeraterEnv.showDone = berater_show_done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EYaTAvAyYO-U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Register with OpenAI Gym"
      ]
    },
    {
      "metadata": {
        "id": "yhI9abUVYNrU",
        "colab_type": "code",
        "outputId": "5044b74b-cfb7-44dc-d0d1-a267f53b1744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "if not 'isEnvRegistered' in locals():\n",
        "  env_id=\"Berater-v1\"\n",
        "  gym.envs.registration.register(id=env_id,entry_point=BeraterEnv,max_episode_steps=1000)\n",
        "  isEnvRegistered=True\n",
        "  print(\"Berater registered as '\" + env_id + \"'\")\n",
        "else:\n",
        "  print(\"Already registered\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berater registered as 'Berater-v1'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sX8eJGcbOJ30",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF-Agents: Train, enjoy, evaluate"
      ]
    },
    {
      "metadata": {
        "id": "bzoq0VM85p46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports & Helpers"
      ]
    },
    {
      "metadata": {
        "id": "5ofYknQFRkRT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.agents.dqn import q_network\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import trajectory\n",
        "from tf_agents.metrics import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.utils import common\n",
        "\n",
        "tf.compat.v1.enable_v2_behavior()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KiP6UgA65163",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NZoGMeUBr2tx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup tf-agent envs, models, policies & agents"
      ]
    },
    {
      "metadata": {
        "id": "sldmm0LmcINW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env_name=\"Berater-v1\"\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IYOUS69Mh3tj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q_net = q_network.QNetwork(\n",
        "    train_env.observation_spec(),\n",
        "    train_env.action_spec(),\n",
        "    fc_layer_params=fc_layer_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9FuiFfYjh9-L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.compat.v2.Variable(0)\n",
        "\n",
        "tf_agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "tf_agent.initialize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hss9CFs2iEt9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eval_policy = tf_agent.policy\n",
        "collect_policy = tf_agent.collect_policy\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3CNLGWERiStH",
        "colab_type": "code",
        "outputId": "6b72040d-f474-4a43-c0d0-2997f8e0c39b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "avg_rnd_return = compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "print(\"avgerage return with random policy={}\".format(avg_rnd_return))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avgerage return with random policy=-6.709170341491699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SVcuWB_FsHa8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Setup replay buffer (train_env)"
      ]
    },
    {
      "metadata": {
        "id": "dYdP4HYHrgGq",
        "colab_type": "code",
        "outputId": "0b1a9247-63c8-41e5-88d8-f70e26a6fd8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=tf_agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_capacity)\n",
        "print(\"replay buffer batch_size=\" + str(train_env.batch_size) + \\\n",
        "      \" max_length=\" + str(replay_buffer_capacity) + \\\n",
        "      \" size=\" + str(replay_buffer.gather_all().observation.shape[1])\n",
        "     )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay buffer batch_size=1 max_length=100000 size=0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UD3yzROJrwUv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "def collect_step(environment, policy):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "  # print(\"collect_step: trajectory=\" + str(traj))\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  replay_buffer.add_batch(traj)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KDSOinfuVk7u",
        "colab_type": "code",
        "outputId": "aced6f12-aa53-4ed7-d217-96fcd575cdbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replay_buffer size=0 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZDRID-p0QOKo",
        "colab_type": "code",
        "outputId": "240a6cbb-6f8f-40bb-ff8c-126c4abf32ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"initial_collect_steps=\" + str(initial_collect_steps))\n",
        "for _ in range(initial_collect_steps):\n",
        "  collect_step(train_env, random_policy)\n",
        "\n",
        "replay_buffer_size=replay_buffer.gather_all().observation.shape[1]\n",
        "print(\"replay_buffer size=\" + str(replay_buffer_size) + \" capacity=\" + str(replay_buffer_capacity))\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial_collect_steps=1000\n",
            "replay_buffer size=1000 capacity=100000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bCHhtAfPsk9H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bwMsD8ATcBBJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# print(iterator.next()[0].action)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myOp6C8ks0Qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the agent"
      ]
    },
    {
      "metadata": {
        "id": "YnptN6_dNJBL",
        "colab_type": "code",
        "outputId": "8634e170-5f51-4aeb-d3be-67f417314ea5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "tf_agent.train = common.function(tf_agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "tf_agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "print(\"retuns={} (before training) num_eval_episodes={}\".format(returns,num_eval_episodes))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "retuns=[-21.77914] (before training) num_eval_episodes=10\n",
            "CPU times: user 7.18 s, sys: 189 ms, total: 7.37 s\n",
            "Wall time: 7.28 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ui4l-FVIsyxp",
        "colab_type": "code",
        "outputId": "f766f10d-51b8-4d95-ecf5-97939ad60c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10242
        }
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "%%time\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect one step using collect_policy and save to the replay buffer.\n",
        "  for _ in range(collect_steps_per_iteration):\n",
        "    collect_step(train_env, tf_agent.collect_policy)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = tf_agent.train(experience)\n",
        "\n",
        "  step = tf_agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step = 10: loss = 6726.234375\n",
            "step = 20: loss = 2694.800048828125\n",
            "step = 20: Average Return = -30.414968490600586\n",
            "step = 30: loss = 6027.49609375\n",
            "step = 40: loss = 8175.3154296875\n",
            "step = 40: Average Return = -33.336631774902344\n",
            "step = 50: loss = 9743.287109375\n",
            "step = 60: loss = 39826.28515625\n",
            "step = 60: Average Return = -33.47330093383789\n",
            "step = 70: loss = 12412.1025390625\n",
            "step = 80: loss = 175252.6875\n",
            "step = 80: Average Return = -33.45663070678711\n",
            "step = 90: loss = 110311.5\n",
            "step = 100: loss = 22539.646484375\n",
            "step = 100: Average Return = -33.41329574584961\n",
            "step = 110: loss = 28311.3984375\n",
            "step = 120: loss = 87142.5703125\n",
            "step = 120: Average Return = -33.49996566772461\n",
            "step = 130: loss = 64616.53515625\n",
            "step = 140: loss = 90741.609375\n",
            "step = 140: Average Return = -33.49996566772461\n",
            "step = 150: loss = 69795.6171875\n",
            "step = 160: loss = 238767.8125\n",
            "step = 160: Average Return = -33.49996566772461\n",
            "step = 170: loss = 58194.87109375\n",
            "step = 180: loss = 563739.25\n",
            "step = 180: Average Return = -33.49996566772461\n",
            "step = 190: loss = 326742.09375\n",
            "step = 200: loss = 112657.7109375\n",
            "step = 200: Average Return = -33.49996566772461\n",
            "step = 210: loss = 161915.5\n",
            "step = 220: loss = 195194.65625\n",
            "step = 220: Average Return = -33.49996566772461\n",
            "step = 230: loss = 7030578.5\n",
            "step = 240: loss = 357728.0\n",
            "step = 240: Average Return = -33.49996566772461\n",
            "step = 250: loss = 4929688.5\n",
            "step = 260: loss = 2469256.5\n",
            "step = 260: Average Return = -33.49996566772461\n",
            "step = 270: loss = 2378154.25\n",
            "step = 280: loss = 273419.875\n",
            "step = 280: Average Return = -33.49996566772461\n",
            "step = 290: loss = 854871.75\n",
            "step = 300: loss = 515032.40625\n",
            "step = 300: Average Return = -33.49996566772461\n",
            "step = 310: loss = 7524177.0\n",
            "step = 320: loss = 280721.1875\n",
            "step = 320: Average Return = -31.12163734436035\n",
            "step = 330: loss = 389016.1875\n",
            "step = 340: loss = 448550.1875\n",
            "step = 340: Average Return = -33.49996566772461\n",
            "step = 350: loss = 353272.34375\n",
            "step = 360: loss = 532993.8125\n",
            "step = 360: Average Return = -33.49996566772461\n",
            "step = 370: loss = 189526.015625\n",
            "step = 380: loss = 508149.75\n",
            "step = 380: Average Return = -33.49996566772461\n",
            "step = 390: loss = 229004.28125\n",
            "step = 400: loss = 778774.5625\n",
            "step = 400: Average Return = -33.49996566772461\n",
            "step = 410: loss = 1686549.25\n",
            "step = 420: loss = 1649020.125\n",
            "step = 420: Average Return = -33.49996566772461\n",
            "step = 430: loss = 1853635.25\n",
            "step = 440: loss = 43228056.0\n",
            "step = 440: Average Return = -33.45496368408203\n",
            "step = 450: loss = 933222.875\n",
            "step = 460: loss = 453491.1875\n",
            "step = 460: Average Return = -33.49996566772461\n",
            "step = 470: loss = 782123.4375\n",
            "step = 480: loss = 891514.875\n",
            "step = 480: Average Return = -33.49996566772461\n",
            "step = 490: loss = 1066217.0\n",
            "step = 500: loss = 951611.625\n",
            "step = 500: Average Return = -33.49996566772461\n",
            "step = 510: loss = 176052.4375\n",
            "step = 520: loss = 24139196.0\n",
            "step = 520: Average Return = -33.49996566772461\n",
            "step = 530: loss = 3808069.0\n",
            "step = 540: loss = 1968140.75\n",
            "step = 540: Average Return = -33.49996566772461\n",
            "step = 550: loss = 901953.5\n",
            "step = 560: loss = 803495.9375\n",
            "step = 560: Average Return = -33.49996566772461\n",
            "step = 570: loss = 617751.875\n",
            "step = 580: loss = 3204393.5\n",
            "step = 580: Average Return = -33.49996566772461\n",
            "step = 590: loss = 1304019.75\n",
            "step = 600: loss = 1634797.25\n",
            "step = 600: Average Return = -33.49996566772461\n",
            "step = 610: loss = 1770604.75\n",
            "step = 620: loss = 1282459.75\n",
            "step = 620: Average Return = -33.49996566772461\n",
            "step = 630: loss = 1287373.75\n",
            "step = 640: loss = 9589021.0\n",
            "step = 640: Average Return = -33.49996566772461\n",
            "step = 650: loss = 48882764.0\n",
            "step = 660: loss = 2612518.25\n",
            "step = 660: Average Return = -33.47163391113281\n",
            "step = 670: loss = 11032104.0\n",
            "step = 680: loss = 813884.9375\n",
            "step = 680: Average Return = -33.49996566772461\n",
            "step = 690: loss = 668969.125\n",
            "step = 700: loss = 1287759.5\n",
            "step = 700: Average Return = -33.49996566772461\n",
            "step = 710: loss = 1361813.0\n",
            "step = 720: loss = 35762012.0\n",
            "step = 720: Average Return = -33.49996566772461\n",
            "step = 730: loss = 33607504.0\n",
            "step = 740: loss = 2276329.0\n",
            "step = 740: Average Return = -33.49996566772461\n",
            "step = 750: loss = 1971008.75\n",
            "step = 760: loss = 1220966.75\n",
            "step = 760: Average Return = -33.49996566772461\n",
            "step = 770: loss = 1456441.5\n",
            "step = 780: loss = 1488042.0\n",
            "step = 780: Average Return = -33.49996566772461\n",
            "step = 790: loss = 3065356.25\n",
            "step = 800: loss = 836756.5\n",
            "step = 800: Average Return = -33.49996566772461\n",
            "step = 810: loss = 3026486.0\n",
            "step = 820: loss = 2073232.5\n",
            "step = 820: Average Return = -33.49996566772461\n",
            "step = 830: loss = 26238790.0\n",
            "step = 840: loss = 1739403.375\n",
            "step = 840: Average Return = -33.49996566772461\n",
            "step = 850: loss = 1591321.875\n",
            "step = 860: loss = 801981.4375\n",
            "step = 860: Average Return = -33.49996566772461\n",
            "step = 870: loss = 965581.3125\n",
            "step = 880: loss = 530620.3125\n",
            "step = 880: Average Return = -33.49996566772461\n",
            "step = 890: loss = 1670954.75\n",
            "step = 900: loss = 1391559.0\n",
            "step = 900: Average Return = -33.49996566772461\n",
            "step = 910: loss = 3270161.5\n",
            "step = 920: loss = 35717892.0\n",
            "step = 920: Average Return = -33.49996566772461\n",
            "step = 930: loss = 1502775.75\n",
            "step = 940: loss = 242782.84375\n",
            "step = 940: Average Return = -33.49996566772461\n",
            "step = 950: loss = 93980200.0\n",
            "step = 960: loss = 1522451.75\n",
            "step = 960: Average Return = -33.49996566772461\n",
            "step = 970: loss = 521768.71875\n",
            "step = 980: loss = 2593460.0\n",
            "step = 980: Average Return = -33.49996566772461\n",
            "step = 990: loss = 360623.3125\n",
            "step = 1000: loss = 458900.9375\n",
            "step = 1000: Average Return = -33.49996566772461\n",
            "step = 1010: loss = 29104926.0\n",
            "step = 1020: loss = 1614625.25\n",
            "step = 1020: Average Return = -33.49996566772461\n",
            "step = 1030: loss = 185019.265625\n",
            "step = 1040: loss = 4121866.0\n",
            "step = 1040: Average Return = -31.133304595947266\n",
            "step = 1050: loss = 1668272.125\n",
            "step = 1060: loss = 2766552.5\n",
            "step = 1060: Average Return = -33.49996566772461\n",
            "step = 1070: loss = 1767498.625\n",
            "step = 1080: loss = 2794228.0\n",
            "step = 1080: Average Return = -28.781641006469727\n",
            "step = 1090: loss = 1548554.125\n",
            "step = 1100: loss = 864222.0\n",
            "step = 1100: Average Return = -33.49996566772461\n",
            "step = 1110: loss = 2707474.0\n",
            "step = 1120: loss = 194693.03125\n",
            "step = 1120: Average Return = -33.47163009643555\n",
            "step = 1130: loss = 21038672.0\n",
            "step = 1140: loss = 569383.6875\n",
            "step = 1140: Average Return = -33.49996566772461\n",
            "step = 1150: loss = 779847.25\n",
            "step = 1160: loss = 637289.875\n",
            "step = 1160: Average Return = -33.49996566772461\n",
            "step = 1170: loss = 1303240.25\n",
            "step = 1180: loss = 1467949.875\n",
            "step = 1180: Average Return = -33.49996566772461\n",
            "step = 1190: loss = 2524030.0\n",
            "step = 1200: loss = 37605704.0\n",
            "step = 1200: Average Return = -33.45496368408203\n",
            "step = 1210: loss = 411092.25\n",
            "step = 1220: loss = 3634176.0\n",
            "step = 1220: Average Return = -33.49996566772461\n",
            "step = 1230: loss = 2982429.5\n",
            "step = 1240: loss = 63232900.0\n",
            "step = 1240: Average Return = -33.49996566772461\n",
            "step = 1250: loss = 322573.0625\n",
            "step = 1260: loss = 46731416.0\n",
            "step = 1260: Average Return = -33.49996566772461\n",
            "step = 1270: loss = 687889.9375\n",
            "step = 1280: loss = 1181137.875\n",
            "step = 1280: Average Return = -33.49996566772461\n",
            "step = 1290: loss = 3165981.0\n",
            "step = 1300: loss = 2523242.25\n",
            "step = 1300: Average Return = -33.47163009643555\n",
            "step = 1310: loss = 202915728.0\n",
            "step = 1320: loss = 33293506.0\n",
            "step = 1320: Average Return = -31.069971084594727\n",
            "step = 1330: loss = 538327.5\n",
            "step = 1340: loss = 88692800.0\n",
            "step = 1340: Average Return = -33.49996566772461\n",
            "step = 1350: loss = 63101560.0\n",
            "step = 1360: loss = 92987456.0\n",
            "step = 1360: Average Return = -33.47163391113281\n",
            "step = 1370: loss = 13862386.0\n",
            "step = 1380: loss = 73935328.0\n",
            "step = 1380: Average Return = -26.369976043701172\n",
            "step = 1390: loss = 397163.8125\n",
            "step = 1400: loss = 45978184.0\n",
            "step = 1400: Average Return = -33.47163391113281\n",
            "step = 1410: loss = 545967.75\n",
            "step = 1420: loss = 2849096.5\n",
            "step = 1420: Average Return = -33.49996566772461\n",
            "step = 1430: loss = 728456.125\n",
            "step = 1440: loss = 1337720.125\n",
            "step = 1440: Average Return = -31.154972076416016\n",
            "step = 1450: loss = 406377.53125\n",
            "step = 1460: loss = 691634.1875\n",
            "step = 1460: Average Return = -31.126638412475586\n",
            "step = 1470: loss = 2087261.5\n",
            "step = 1480: loss = 1609653.0\n",
            "step = 1480: Average Return = -31.05997085571289\n",
            "step = 1490: loss = 481287.75\n",
            "step = 1500: loss = 404270.65625\n",
            "step = 1500: Average Return = -33.45496368408203\n",
            "step = 1510: loss = 104794552.0\n",
            "step = 1520: loss = 33820800.0\n",
            "step = 1520: Average Return = -26.464981079101562\n",
            "step = 1530: loss = 98506416.0\n",
            "step = 1540: loss = 637950.875\n",
            "step = 1540: Average Return = -33.45496368408203\n",
            "step = 1550: loss = 571872.0\n",
            "step = 1560: loss = 435556.96875\n",
            "step = 1560: Average Return = -33.49996566772461\n",
            "step = 1570: loss = 86000784.0\n",
            "step = 1580: loss = 2397007.0\n",
            "step = 1580: Average Return = -33.39829635620117\n",
            "step = 1590: loss = 797881.5625\n",
            "step = 1600: loss = 657539.875\n",
            "step = 1600: Average Return = -26.363311767578125\n",
            "step = 1610: loss = 387079.6875\n",
            "step = 1620: loss = 620513.8125\n",
            "step = 1620: Average Return = -33.49996566772461\n",
            "step = 1630: loss = 363957.90625\n",
            "step = 1640: loss = 1424278.375\n",
            "step = 1640: Average Return = -26.419979095458984\n",
            "step = 1650: loss = 2327887.5\n",
            "step = 1660: loss = 243617.328125\n",
            "step = 1660: Average Return = -28.80997657775879\n",
            "step = 1670: loss = 436187.46875\n",
            "step = 1680: loss = 1336662.375\n",
            "step = 1680: Average Return = -33.49996566772461\n",
            "step = 1690: loss = 540684.1875\n",
            "step = 1700: loss = 358601.09375\n",
            "step = 1700: Average Return = -33.49996566772461\n",
            "step = 1710: loss = 551330.75\n",
            "step = 1720: loss = 806360.9375\n",
            "step = 1720: Average Return = -33.49996566772461\n",
            "step = 1730: loss = 83976224.0\n",
            "step = 1740: loss = 1382668.0\n",
            "step = 1740: Average Return = -26.464981079101562\n",
            "step = 1750: loss = 2176573.5\n",
            "step = 1760: loss = 1333699.25\n",
            "step = 1760: Average Return = -33.47163391113281\n",
            "step = 1770: loss = 1952157.375\n",
            "step = 1780: loss = 294781.84375\n",
            "step = 1780: Average Return = -28.139972686767578\n",
            "step = 1790: loss = 235330544.0\n",
            "step = 1800: loss = 1049582.625\n",
            "step = 1800: Average Return = -21.753326416015625\n",
            "step = 1810: loss = 1901059.0\n",
            "step = 1820: loss = 461857.75\n",
            "step = 1820: Average Return = -18.71499252319336\n",
            "step = 1830: loss = 610918.0\n",
            "step = 1840: loss = 3094154.5\n",
            "step = 1840: Average Return = -28.80997657775879\n",
            "step = 1850: loss = 501456.0\n",
            "step = 1860: loss = 657675.125\n",
            "step = 1860: Average Return = -31.0132999420166\n",
            "step = 1870: loss = 95688216.0\n",
            "step = 1880: loss = 796944.875\n",
            "step = 1880: Average Return = -31.133304595947266\n",
            "step = 1890: loss = 817126.5\n",
            "step = 1900: loss = 861748.625\n",
            "step = 1900: Average Return = -31.09330177307129\n",
            "step = 1910: loss = 835434.75\n",
            "step = 1920: loss = 2149247.5\n",
            "step = 1920: Average Return = -33.381629943847656\n",
            "step = 1930: loss = 11237946.0\n",
            "step = 1940: loss = 1791417.0\n",
            "step = 1940: Average Return = -25.744979858398438\n",
            "step = 1950: loss = 280406.28125\n",
            "step = 1960: loss = 2515848.25\n",
            "step = 1960: Average Return = -25.73830795288086\n",
            "step = 1970: loss = 909642.625\n",
            "step = 1980: loss = 194335.15625\n",
            "step = 1980: Average Return = -33.41496658325195\n",
            "step = 1990: loss = 1816232.25\n",
            "step = 2000: loss = 65670900.0\n",
            "step = 2000: Average Return = -17.41999053955078\n",
            "step = 2010: loss = 2396078.75\n",
            "step = 2020: loss = 1444577.75\n",
            "step = 2020: Average Return = -33.42662811279297\n",
            "step = 2030: loss = 338113.875\n",
            "step = 2040: loss = 889901.625\n",
            "step = 2040: Average Return = -30.38330078125\n",
            "step = 2050: loss = 198914.40625\n",
            "step = 2060: loss = 476493.875\n",
            "step = 2060: Average Return = -28.80997657775879\n",
            "step = 2070: loss = 49096132.0\n",
            "step = 2080: loss = 44560048.0\n",
            "step = 2080: Average Return = -26.464981079101562\n",
            "step = 2090: loss = 74859920.0\n",
            "step = 2100: loss = 124253752.0\n",
            "step = 2100: Average Return = -33.44329833984375\n",
            "step = 2110: loss = 798163.375\n",
            "step = 2120: loss = 69701168.0\n",
            "step = 2120: Average Return = -31.109970092773438\n",
            "step = 2130: loss = 2087283.25\n",
            "step = 2140: loss = 63640520.0\n",
            "step = 2140: Average Return = -33.47163009643555\n",
            "step = 2150: loss = 347171.6875\n",
            "step = 2160: loss = 420548.0\n",
            "step = 2160: Average Return = -33.49996566772461\n",
            "step = 2170: loss = 549927.75\n",
            "step = 2180: loss = 847844.0\n",
            "step = 2180: Average Return = -18.73166275024414\n",
            "step = 2190: loss = 4131619.5\n",
            "step = 2200: loss = 821910.8125\n",
            "step = 2200: Average Return = -23.44998550415039\n",
            "step = 2210: loss = 29218408.0\n",
            "step = 2220: loss = 923258.75\n",
            "step = 2220: Average Return = -31.014965057373047\n",
            "step = 2230: loss = 496547.15625\n",
            "step = 2240: loss = 543247.125\n",
            "step = 2240: Average Return = -31.104970932006836\n",
            "step = 2250: loss = 209014.84375\n",
            "step = 2260: loss = 564998.0\n",
            "step = 2260: Average Return = -26.386646270751953\n",
            "step = 2270: loss = 629721.625\n",
            "step = 2280: loss = 503287.59375\n",
            "step = 2280: Average Return = -21.718324661254883\n",
            "step = 2290: loss = 35742988.0\n",
            "step = 2300: loss = 1624042.75\n",
            "step = 2300: Average Return = -31.109970092773438\n",
            "step = 2310: loss = 92330784.0\n",
            "step = 2320: loss = 809549.6875\n",
            "step = 2320: Average Return = -18.061660766601562\n",
            "step = 2330: loss = 1010702.0\n",
            "step = 2340: loss = 443513.375\n",
            "step = 2340: Average Return = -28.736642837524414\n",
            "step = 2350: loss = 1451692.25\n",
            "step = 2360: loss = 3342556.0\n",
            "step = 2360: Average Return = -19.736650466918945\n",
            "step = 2370: loss = 76999624.0\n",
            "step = 2380: loss = 699066.75\n",
            "step = 2380: Average Return = -18.649993896484375\n",
            "step = 2390: loss = 4294987.5\n",
            "step = 2400: loss = 263641.25\n",
            "step = 2400: Average Return = -28.781641006469727\n",
            "step = 2410: loss = 72804512.0\n",
            "step = 2420: loss = 460195.875\n",
            "step = 2420: Average Return = -25.74997901916504\n",
            "step = 2430: loss = 1179566.75\n",
            "step = 2440: loss = 639764.375\n",
            "step = 2440: Average Return = -25.773311614990234\n",
            "step = 2450: loss = 747780.125\n",
            "step = 2460: loss = 299932.0625\n",
            "step = 2460: Average Return = -33.45496368408203\n",
            "step = 2470: loss = 51806660.0\n",
            "step = 2480: loss = 602728.1875\n",
            "step = 2480: Average Return = -31.154972076416016\n",
            "step = 2490: loss = 1019370.375\n",
            "step = 2500: loss = 2304078.75\n",
            "step = 2500: Average Return = -21.77499008178711\n",
            "step = 2510: loss = 3090146.5\n",
            "step = 2520: loss = 14076805.0\n",
            "step = 2520: Average Return = -31.014968872070312\n",
            "step = 2530: loss = 995100.375\n",
            "step = 2540: loss = 335345.0625\n",
            "step = 2540: Average Return = -28.781641006469727\n",
            "step = 2550: loss = 486968.90625\n",
            "step = 2560: loss = 2810042.75\n",
            "step = 2560: Average Return = -33.49996566772461\n",
            "step = 2570: loss = 507646.875\n",
            "step = 2580: loss = 771172.875\n",
            "step = 2580: Average Return = -33.49996566772461\n",
            "step = 2590: loss = 2194922.75\n",
            "step = 2600: loss = 188970.46875\n",
            "step = 2600: Average Return = -20.339984893798828\n",
            "step = 2610: loss = 43497260.0\n",
            "step = 2620: loss = 228780.375\n",
            "step = 2620: Average Return = -21.77499008178711\n",
            "step = 2630: loss = 2071693.5\n",
            "step = 2640: loss = 357848.4375\n",
            "step = 2640: Average Return = -33.381629943847656\n",
            "step = 2650: loss = 3002637.75\n",
            "step = 2660: loss = 233563.34375\n",
            "step = 2660: Average Return = -24.119985580444336\n",
            "step = 2670: loss = 51570352.0\n",
            "step = 2680: loss = 520353.03125\n",
            "step = 2680: Average Return = -31.104970932006836\n",
            "step = 2690: loss = 206267.09375\n",
            "step = 2700: loss = 273361.8125\n",
            "step = 2700: Average Return = -16.404998779296875\n",
            "step = 2710: loss = 3083966.5\n",
            "step = 2720: loss = 431173.375\n",
            "step = 2720: Average Return = -14.69500732421875\n",
            "step = 2730: loss = 29979196.0\n",
            "step = 2740: loss = 3643238.5\n",
            "step = 2740: Average Return = -31.126636505126953\n",
            "step = 2750: loss = 403909.75\n",
            "step = 2760: loss = 308416.40625\n",
            "step = 2760: Average Return = -33.45496368408203\n",
            "step = 2770: loss = 19847194.0\n",
            "step = 2780: loss = 324872.21875\n",
            "step = 2780: Average Return = -23.376651763916016\n",
            "step = 2790: loss = 429138.875\n",
            "step = 2800: loss = 277512.1875\n",
            "step = 2800: Average Return = -28.78164291381836\n",
            "step = 2810: loss = 233977.875\n",
            "step = 2820: loss = 577223.375\n",
            "step = 2820: Average Return = -18.68332862854004\n",
            "step = 2830: loss = 330882.59375\n",
            "step = 2840: loss = 291250.25\n",
            "step = 2840: Average Return = -33.47163391113281\n",
            "step = 2850: loss = 116000.09375\n",
            "step = 2860: loss = 177241456.0\n",
            "step = 2860: Average Return = -30.484970092773438\n",
            "step = 2870: loss = 428769.75\n",
            "step = 2880: loss = 49859180.0\n",
            "step = 2880: Average Return = -28.47330665588379\n",
            "step = 2890: loss = 558381.375\n",
            "step = 2900: loss = 36217096.0\n",
            "step = 2900: Average Return = -21.034990310668945\n",
            "step = 2910: loss = 1549090.125\n",
            "step = 2920: loss = 83660136.0\n",
            "step = 2920: Average Return = -33.49996566772461\n",
            "step = 2930: loss = 11045113.0\n",
            "step = 2940: loss = 18032496.0\n",
            "step = 2940: Average Return = -26.253311157226562\n",
            "step = 2950: loss = 609666.125\n",
            "step = 2960: loss = 732553.125\n",
            "step = 2960: Average Return = -33.426631927490234\n",
            "step = 2970: loss = 1460320.125\n",
            "step = 2980: loss = 516114.46875\n",
            "step = 2980: Average Return = -33.49996566772461\n",
            "step = 2990: loss = 400720.8125\n",
            "step = 3000: loss = 96520.484375\n",
            "step = 3000: Average Return = -31.031635284423828\n",
            "step = 3010: loss = 227178.421875\n",
            "step = 3020: loss = 191838.0625\n",
            "step = 3020: Average Return = -33.47163009643555\n",
            "step = 3030: loss = 2405269.25\n",
            "step = 3040: loss = 1880181.0\n",
            "step = 3040: Average Return = -28.80997657775879\n",
            "step = 3050: loss = 1114487.5\n",
            "step = 3060: loss = 281335.90625\n",
            "step = 3060: Average Return = -21.75332260131836\n",
            "step = 3070: loss = 732705.5625\n",
            "step = 3080: loss = 65792564.0\n",
            "step = 3080: Average Return = -26.3983154296875\n",
            "step = 3090: loss = 339266.8125\n",
            "step = 3100: loss = 274535.1875\n",
            "step = 3100: Average Return = -28.76497459411621\n",
            "step = 3110: loss = 561067.375\n",
            "step = 3120: loss = 11261259.0\n",
            "step = 3120: Average Return = -33.44329833984375\n",
            "step = 3130: loss = 269658.15625\n",
            "step = 3140: loss = 10306756.0\n",
            "step = 3140: Average Return = -33.49996566772461\n",
            "step = 3150: loss = 159166.359375\n",
            "step = 3160: loss = 399137.5\n",
            "step = 3160: Average Return = -31.014968872070312\n",
            "step = 3170: loss = 232646.3125\n",
            "step = 3180: loss = 910896.9375\n",
            "step = 3180: Average Return = -33.49996566772461\n",
            "step = 3190: loss = 279839.5\n",
            "step = 3200: loss = 597890.0\n",
            "step = 3200: Average Return = -18.70999526977539\n",
            "step = 3210: loss = 2395476.5\n",
            "step = 3220: loss = 155652.09375\n",
            "step = 3220: Average Return = -31.053302764892578\n",
            "step = 3230: loss = 273690.4375\n",
            "step = 3240: loss = 168905.828125\n",
            "step = 3240: Average Return = -24.02498435974121\n",
            "step = 3250: loss = 95438288.0\n",
            "step = 3260: loss = 397385.1875\n",
            "step = 3260: Average Return = -19.401662826538086\n",
            "step = 3270: loss = 468526.6875\n",
            "step = 3280: loss = 430010.40625\n",
            "step = 3280: Average Return = -18.738327026367188\n",
            "step = 3290: loss = 357773.0\n",
            "step = 3300: loss = 680621.3125\n",
            "step = 3300: Average Return = -31.088302612304688\n",
            "step = 3310: loss = 9936669.0\n",
            "step = 3320: loss = 392720.75\n",
            "step = 3320: Average Return = -30.679967880249023\n",
            "step = 3330: loss = 21312040.0\n",
            "step = 3340: loss = 382358.875\n",
            "step = 3340: Average Return = -21.696659088134766\n",
            "step = 3350: loss = 3503372.0\n",
            "step = 3360: loss = 1435751.0\n",
            "step = 3360: Average Return = -33.49996566772461\n",
            "step = 3370: loss = 40862380.0\n",
            "step = 3380: loss = 498876.625\n",
            "step = 3380: Average Return = -31.088302612304688\n",
            "step = 3390: loss = 231579.375\n",
            "step = 3400: loss = 738763.4375\n",
            "step = 3400: Average Return = -21.746654510498047\n",
            "step = 3410: loss = 1543599.875\n",
            "step = 3420: loss = 342489.5625\n",
            "step = 3420: Average Return = -24.034984588623047\n",
            "step = 3430: loss = 287713.125\n",
            "step = 3440: loss = 215182.734375\n",
            "step = 3440: Average Return = -33.39829635620117\n",
            "step = 3450: loss = 881451.125\n",
            "step = 3460: loss = 167281.65625\n",
            "step = 3460: Average Return = -33.39829635620117\n",
            "step = 3470: loss = 2480775.5\n",
            "step = 3480: loss = 329448.625\n",
            "step = 3480: Average Return = -28.753307342529297\n",
            "step = 3490: loss = 734662.375\n",
            "step = 3500: loss = 1072427.0\n",
            "step = 3500: Average Return = -16.41499900817871\n",
            "step = 3510: loss = 22603326.0\n",
            "step = 3520: loss = 221792.96875\n",
            "step = 3520: Average Return = -33.39829635620117\n",
            "step = 3530: loss = 21524204.0\n",
            "step = 3540: loss = 982046.25\n",
            "step = 3540: Average Return = -33.49996566772461\n",
            "step = 3550: loss = 657439.75\n",
            "step = 3560: loss = 927760.375\n",
            "step = 3560: Average Return = -14.661674499511719\n",
            "step = 3570: loss = 331235.96875\n",
            "step = 3580: loss = 713022.0625\n",
            "step = 3580: Average Return = -21.696659088134766\n",
            "step = 3590: loss = 406945.1875\n",
            "step = 3600: loss = 184478.296875\n",
            "step = 3600: Average Return = -33.45496368408203\n",
            "step = 3610: loss = 259724.5\n",
            "step = 3620: loss = 337482.5\n",
            "step = 3620: Average Return = -31.059967041015625\n",
            "step = 3630: loss = 198592.953125\n",
            "step = 3640: loss = 215175.1875\n",
            "step = 3640: Average Return = -31.014965057373047\n",
            "step = 3650: loss = 652123.5\n",
            "step = 3660: loss = 856292.5\n",
            "step = 3660: Average Return = -28.771642684936523\n",
            "step = 3670: loss = 503734.9375\n",
            "step = 3680: loss = 54213104.0\n",
            "step = 3680: Average Return = -33.49996566772461\n",
            "step = 3690: loss = 554418.0625\n",
            "step = 3700: loss = 144856.5\n",
            "step = 3700: Average Return = -31.109970092773438\n",
            "step = 3710: loss = 276525.375\n",
            "step = 3720: loss = 381894.15625\n",
            "step = 3720: Average Return = -28.804973602294922\n",
            "step = 3730: loss = 290510.0625\n",
            "step = 3740: loss = 29617642.0\n",
            "step = 3740: Average Return = -17.056669235229492\n",
            "step = 3750: loss = 130756.609375\n",
            "step = 3760: loss = 35156132.0\n",
            "step = 3760: Average Return = -26.4316463470459\n",
            "step = 3770: loss = 203251.640625\n",
            "step = 3780: loss = 407344.8125\n",
            "step = 3780: Average Return = -26.431650161743164\n",
            "step = 3790: loss = 502167.0\n",
            "step = 3800: loss = 20478086.0\n",
            "step = 3800: Average Return = -23.423315048217773\n",
            "step = 3810: loss = 185137.765625\n",
            "step = 3820: loss = 172529.59375\n",
            "step = 3820: Average Return = -19.393329620361328\n",
            "step = 3830: loss = 9554146.0\n",
            "step = 3840: loss = 26485920.0\n",
            "step = 3840: Average Return = -28.75497055053711\n",
            "step = 3850: loss = 1353272.5\n",
            "step = 3860: loss = 149403.328125\n",
            "step = 3860: Average Return = -31.059967041015625\n",
            "step = 3870: loss = 1483062.125\n",
            "step = 3880: loss = 266463.4375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ty7ul0Lk6nou",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualize training rewards"
      ]
    },
    {
      "metadata": {
        "id": "6vrAtGt3ck4Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"num_iterations={} eval_interval={} tf_agent: train_step_counter={} train_sequence_length={}\".format(\\\n",
        "                 num_iterations, \\\n",
        "                 eval_interval, \\\n",
        "                 tf_agent.train_step_counter.numpy(), \\\n",
        "                 tf_agent.train_sequence_length))\n",
        "print(\"returns={}\".format(returns))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58gwqW92M_d_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@test {\"skip\": true}\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "steps = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(steps, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Step')\n",
        "plt.ylim(top=250)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}